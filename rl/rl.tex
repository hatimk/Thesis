\chapter{Reinforcement learning for turn-taking optimisation}
\label{ch:rl}

\section{Model}

	\subsection{State representation}
    
    	The following features are used to describe the system state:
        
        \begin{itemize}
        	\item \textbf{SYSTEM\_REQ:} At each dialogue turn, the system is requiring a particular information. For instance, after an open question, it is waiting for all the slot values to be provided at once but it can also be waiting for a specific question or a response to a confirmation. This feature refers to the information that it is waiting for at this moment. It can take 6 different values.
            \item \textbf{LAST\_INCR\_RESP:} As described in Chap. \ref{ch:architecture}, the Scheduler stores the last response it gets from the service at each micro-turn. We use that as our second feature. 11 different values can be taken by this feature.
            \item \textbf{NB\_USER\_WORDS:} This feature is a counter of the number of words since the last change of LAST\_INCR\_RESP (the number of words since the service did not change its mind about the response to deliver).
            \item \textbf{NORMALISED\_SCORE:} At each micro-turn, the ASR score is updated: it is multiplied by the ASR corresponding to the new incoming word (see Fig. \ref{fig:asrsimu}). Except from the cases where a boost takes place, the score keeps decreasing as the user speaks. To avoid penalising long sentences, the score is normalised by taking the geometric mean over the words (of course, this is an approximation as the number of inputs that formed the current ASR hypothesis is not exactly the number of words because of deletions and additions in the Scrambler). If $s$ is the current score for $n$ number of words, NORMALISED\_SCORE = $s^{\frac{1}{n}}$.
            \item \textbf{TIME:} Corresponds to the duration of the current episode.
        \end{itemize}
        
        %HK> Justifier le choix d'une représentation linéaire
        %HK> Il faut introduire Fitted-Q, dire qu'on a essayé autre chose (tabulaire, Q-Learning...).
        A linear model is used to represent the Q-function. First, we noticed that 21 combinations between SYSTEM\_REQ and LAST\_INCR\_RESP are frequently visited (the others barely happen or not at all). Therefore, 21 features are defined $\delta_1$,...,$\delta_{21}$ where $\delta_i = 1$ if and only if the current state corresponds to the $i^{th}$ combination, and 0 otherwise. The rare combinations are not included in the model for two reasons. First, they require maintaining heavier models for with no real improvements over the simpler ones and second, Fitted-Q algorithm was chosen for resolving the MDP and it involves a matrix inversion that should be well conditioned. Having features that are equal to zero most of the time is not compliant with that condition.
        
        %HK> Utiliser eqnarray
        NB\_USER\_WORDS is represented by three RBF functions $\phi^{nw}_1$, $\phi^{nw}_2$ and $\phi^{nw}_3$ centered in 0, 5 and 10 with a standard deviation of 2, 3 and 3. In other words
        
        	$$ \phi^{nw}_i = \exp (\frac{NB\_USER\_WORDS - \mu_i}{2 \sigma_i^2}) $$
           	$$ \mu_1 = 0, \text{ } \mu_2 = 5, \text{ }, \mu_3 = 10 $$
            $$ \sigma_1 = 2, \text{ } \sigma_2 = 3, \text{ } \sigma_3 = 3 $$
            
      	Similarly, NORMALISED\_SCORE is also represented using two RBF functions $\phi^{ns}_1$ and $\phi^{ns}_2$ centered in 0.25 and 0.75 and with a standard deviation of 0.3 for both.
        
        Finally, TIME is normalised so that it is near zero at the beginning of the episode and around 1 when the duration reaches 6 minutes (the maximum duration due to patience limit):
        
        	$$ T = sigmoid(\frac{(TIME-180)}{60}) $$
            
       	There is no need to use RBFs for this last feature as the Q-function is supposed to be monotonous with respect to it. The more the dialogue last, the more likely the user would hang up.
        
        Therefore, the dialogue state is represented by the following vector
        
        	$$ \Phi(s) = [1,\delta_1,\delta_2,\delta_3,\phi^{nw}_1,\phi^{nw}_2,\phi^{nw}_3,\phi^{ns}_1,\phi^{ns}_2,T] $$
            
   	\subsection{Actions, rewards and episodes}
    
    	%HK> Décrire plus en détail et justifier.
    	The system can perform the action WAIT and the action SPEAK (see Chap. \ref{ch:baseline} for a description of these actions). The action REPEAT introduces more complexity to the system and therefore, it is left for future work.
        
    	At each micro-turn, the system receives a reward $-\Delta t$ which correspond to the opposite of the time elapsed since the micro-turn before. Moreover, there are two rewarding situations, where the system gets a reward of 150:
        
        \begin{itemize}
        	\item The Scheduler retrieves a confirmation that the task corresponding to the user's requests has been accomplished to the client (happens when the user says \textit{yes} to a confirmation question).
            \item The Scheduler retrieves a conflict declaration to the client. Even though the task has not been accomplished from the dialogue manager point of view, it has been from the Scheduler's side.
        \end{itemize}
        
        An episode is a portion of a dialogue that starts with an open question (where the user is supposed to utter a complete request with all the necessary slot values) and ends with either a new open question or a user hang up.
            
	\subsection{Fitted-Q Value Iteration}
    
    	Fitted-Q iteration has already been successfully applied to dialogue management in the traditional sense \cite{Chandramohan2010}. Here it is applied to the problem of turn-taking. 
            
      	where $\theta(a)$ is a parameter vector associated with action a. For the notations to be more compact, we will omit the action and refer to this parameters vector as $\theta$. The aim of Fitted-Q algorithm is to estimate those parameters for the optimal Q-function $Q^*$. Recall that the Bellman optimality equation states that
        
        	$$ Q^*(s,a) = \mathbb{E}[R(s,a,s') + \gamma \max_{a'} Q^*(s,a') | s, a]$$
            $$ Q^* = T^* Q^*$$
            
       	The operator $T^*$ is a contraction \cite{Bellman1957}. As a consequence, there is a way to estimate it in an iterative way: \textit{Value Iteration} (Banach theorem). Each new iteration is linked to the previous one as follows:
        
        	$$ \hat{Q}^*_i = T^* \hat{Q}^*_{i-1}$$
            
       	However, an exact representation of the Q-function is assumed which is not possible in our case as the state space is infinite (even if we discretise the ASR score, it will still be too big). As a consequence, we adopt a linear representation of the Q-function:
        
        	$$ \hat{Q}(s,a) = \theta(a)^T \Phi(s) $$
            
      	$\hat{Q}$ is the projection of Q on the space of the functions that can be written as a linear combination of the state vector's components. Let $\Pi$ be the corresponding projection operator, then it can be shown that $\Pi T^*$ is still a contraction and admits a unique fixed point that can be iteratively computed as follows: $\hat{Q}_{\theta_i} = \Pi T^* \hat{Q}_{\theta_{i-1}}$. As the transition probabilities of the MDP and the reward function are not known, a sampled operator $\hat{T}$ is used instead of $T$. For a transition (s,a,r,s'), it is defined as
        
        	$$ \hat{T} \hat{Q}(s,a) = r + \gamma \max_a' Q(s',a') $$
            
      	The Fitted-Q algorithm therefore estimates the $\theta$ vector using the iteration rule: $\hat{Q}_{\theta_i} = \hat{T} \hat{Q}_{\theta_{i-1}}$. To compute the projection, the $L_2$ norm is minimised:
        
        	$$ \theta_i = \argmin_{\theta \in \mathbb{R}^p} \sum_{j=1}^N (r_j + \gamma \max_a \theta_{i-1}(a)^T \phi{s_j} - \theta (a_j) \phi(s_j)) $$
            
      	where N is the number of transitions in the data batch. This is a classic least square optimisation and $\theta_i$ can be computed as follows (the matrix inversion has to be performed only once and not at every iteration):
        
        	$$ \theta_i = (\sum_{i=1}^N \phi(s_i)^T \phi(s_i))^{-1} \sum_{j=1}^N \phi(s_j) (r_j + \gamma \max_a \theta_{i-1}(a)^T \phi(s_j)) $$
            

\section{Experiment}
	
    \subsection{Setup}

		The same dialogue scenarios described in Chap. \ref{ch:baseline} are used here. For learning, the noise level is fixed at 0.15. 50 learning curves have been produced with 3000 episodes each and the average curve is depicted in Fig. \ref{fig:learn}. The $\theta$ vectors in the Q-function model are initially zeros and they are updated every 500 episodes. There are three phases to distinguish:
        
        %HK> Faire une référence au chapitre sur l'optimisation du travail sur le dilemme exploitation/exploration.
        \begin{enumerate}
        	\item \textbf{Pure exploration (Episodes 0-500):} The actions are taken randomly with a probability of 0.8 for choosing WAIT and 0.2 for SPEAK. Picking equiprobable actions results in the user being interrupted so often that the interesting scenarios are very rarely explored.
            \item \textbf{Exploitation/exploration (Episodes 500-2500):} An $\epsilon$-greedy policy is used with respect to the current Q-function, with $\epsilon=0.1$.
            \item \textbf{Pure exploitation (Episodes 2500-3000):} A 100\% greedy policy is used.
        \end{enumerate}
   
   	\subsection{Results}
    
    \begin{figure}[t]
      \centering
      \begin{tikzpicture}[scale=0.8]
      \begin{axis}[
      xlabel={Number of episodes},
      ylabel={Reward},
      scaled ticks = false,
      tick label style={/pgf/number format/fixed},
      xmin=100, xmax=3000,
      ymin=95, ymax=120,
      xtick={100,500,1000,1500,2000,2500,3000},
      ytick={95,100,105,110,115,120},
      legend pos=south east,
      ymajorgrids=true,
      grid style=dashed,
      colorbrewer cycle list=Set1,
      ]

  \addplot+[
  error bars/.cd,
  y dir=both,
  y explicit,
  ]
  coordinates {
  (100,104.5)
  (150,104.5)
  (200,104.5)
  (250,104.5)
  (300,104.5)
  (350,104.5)
  (400,104.5)
  (450,104.5)
  (500,104.5)
  (550,104.5)
  (600,104.5)
  (650,104.5)
  (700,104.5)
  (750,104.5)
  (800,104.5)
  (850,104.5)
  (900,104.5)
  (950,104.5)
  (1000,104.5)
  (1050,104.5)
  (1100,104.5)
  (1150,104.5)
  (1200,104.5)
  (1250,104.5)
  (1300,104.5)
  (1350,104.5)
  (1400,104.5)
  (1450,104.5)
  (1500,104.5)
  (1550,104.5)
  (1600,104.5)
  (1650,104.5)
  (1700,104.5)
  (1750,104.5)
  (1800,104.5)
  (1850,104.5)
  (1900,104.5)
  (1950,104.5)
  (2000,104.5)
  (2050,104.5)
  (2100,104.5)
  (2150,104.5)
  (2200,104.5)
  (2250,104.5)
  (2300,104.5)
  (2350,104.5)
  (2400,104.5)
  (2450,104.5)
  (2500,104.5)
  (2550,104.5)
  (2600,104.5)
  (2650,104.5)
  (2700,104.5)
  (2750,104.5)
  (2800,104.5)
  (2850,104.5)
  (2900,104.5)
  (2950,104.5)
  (3000,104.5)
  };

  \addplot+[
  error bars/.cd,
  y dir=both,
  y explicit,
  ]
  coordinates {
  (100,110)
  (150,110)
  (200,110)
  (250,110)
  (300,110)
  (350,110)
  (400,110)
  (450,110)
  (500,110)
  (550,110)
  (600,110)
  (650,110)
  (700,110)
  (750,110)
  (800,110)
  (850,110)
  (900,110)
  (950,110)
  (1000,110)
  (1050,110)
  (1100,110)
  (1150,110)
  (1200,110)
  (1250,110)
  (1300,110)
  (1350,110)
  (1400,110)
  (1450,110)
  (1500,110)
  (1550,110)
  (1600,110)
  (1650,110)
  (1700,110)
  (1750,110)
  (1800,110)
  (1850,110)
  (1900,110)
  (1950,110)
  (2000,110)
  (2050,110)
  (2100,110)
  (2150,110)
  (2200,110)
  (2250,110)
  (2300,110)
  (2350,110)
  (2400,110)
  (2450,110)
  (2500,110)
  (2550,110)
  (2600,110)
  (2650,110)
  (2700,110)
  (2750,110)
  (2800,110)
  (2850,110)
  (2900,110)
  (2950,110)
  (3000,110)

  };

  \addplot+[
  only marks,
  error bars/.cd,
  y dir=both,
  y explicit,
  ]
  coordinates {
  (100,99.9554)
  (150,100.126)
  (200,100.1544)
  (250,100.3698)
  (300,100.1902)
  (350,100.0802)
  (400,99.6666)
  (450,99.643)
  (500,99.8262)
  (600,111.0898)
  (650,110.619)
  (700,110.4986)
  (750,110.667)
  (800,110.6698)
  (850,110.5932)
  (900,110.6876)
  (950,111.0216)
  (1000,111.2774)
  (1050,110.7884)
  (1100,110.9832)
  (1150,111.0464)
  (1200,110.8506)
  (1250,110.6576)
  (1300,110.903)
  (1350,111.7178)
  (1400,111.428)
  (1450,110.8136)
  (1500,110.7896)
  (1550,110.9168)
  (1600,110.5906)
  (1650,110.4182)
  (1700,110.7948)
  (1750,110.792)
  (1800,110.6854)
  (1850,110.485)
  (1900,110.3116)
  (1950,110.4132)
  (2000,110.2364)
  (2050,110.5788)
  (2100,111.0018)
  (2150,110.7796)
  (2200,111.6892)
  (2250,111.791)
  (2300,110.1606)
  (2350,110.5042)
  (2400,111.7458)
  (2450,111.0896)
  (2500,110.744)
  (2600,114.2912)
  (2650,114.86)
  (2700,114.6676)
  (2750,114.1698)
  (2800,114.3308)
  (2850,114.8298)
  (2900,114.7522)
  (2950,114.0682)
  (3000,113.6722)
  };

  \legend{Non Incremental,Incremental Baseline,Incremental RL}

  \end{axis}
  \end{tikzpicture}
  \caption{Learning curve (0-500: pure exploration, 500-2500: exploration/exploitation, 2500-3000: pure exploitation)}
  \label{fig:learn}
\end{figure}

\begin{figure*}[b]
\centering
\begin{minipage}{.47\textwidth}
		\begin{tikzpicture}[scale=0.8]
    \begin{axis}[
    xlabel={WER},
    ylabel={Mean duration (sec)},
    scaled ticks = false,
    tick label style={/pgf/number format/fixed},
    xmin=0, xmax=0.3,
    ymin=60, ymax=240,
    xtick={0,0.06,0.12,0.18,0.24,0.3},
    ytick={60,90,120,150,180,210,240},
    legend pos=north west,
    ymajorgrids=true,
    grid style=dashed,
    colorbrewer cycle list=Set1,
    ]

  \addplot+[
  error bars/.cd,
  y dir=both,
  y explicit,
  ]
  coordinates {
  (0.0,81.81333333333323) +- (0.49131639706677,0.49131639706677)
  (0.03,91.71533333333338) +- (0.7370071114727688,0.7370071114727688)
  (0.06,100.61999999999999) +- (0.9163154073570945,0.9163154073570945)
  (0.09,110.30933333333337) +- (1.1079317542734202,1.1079317542734202)
  (0.12,120.25133333333329) +- (1.3145078817622518,1.3145078817622518)
  (0.15,128.88766666666666) +- (1.4520881830480932,1.4520881830480932)
  (0.18,139.9666666666667) +- (1.6146051787906812,1.6146051787906812)
  (0.21,148.08833333333314) +- (1.7406157103623126,1.7406157103623126)
  (0.24,160.28433333333334) +- (2.0239947579988615,2.0239947579988615)
  (0.27,170.64066666666687) +- (2.1548187185752323,2.1548187185752323)
  (0.3,179.7433333333333) +- (2.2832048422084354,2.2832048422084354)
  };

  \addplot+[
  error bars/.cd,
  y dir=both,
  y explicit,
  ]
  coordinates {
  (0.0,72.33766666666655) +- (0.19309601399017187,0.19309601399017187)
  (0.03,81.02566666666667) +- (0.5634594051387006,0.5634594051387006)
  (0.06,88.5193333333334) +- (0.6941501827939834,0.6941501827939834)
  (0.09,97.77333333333321) +- (0.9424363565520608,0.9424363565520608)
  (0.12,106.84066666666676) +- (1.0881492743933865,1.0881492743933865)
  (0.15,116.92933333333318) +- (1.2450282893676852,1.2450282893676852)
  (0.18,125.67533333333333) +- (1.460507242474365,1.460507242474365)
  (0.21,137.1566666666666) +- (1.6699996326865956,1.6699996326865956)
  (0.24,145.81366666666656) +- (1.7355578985505493,1.7355578985505493)
  (0.27,156.351) +- (1.9864460266108706,1.9864460266108706)
  (0.3,168.65766666666676) +- (2.124651277704671,2.124651277704671)
  };

  \addplot+[
  error bars/.cd,
  y dir=both,
  y explicit,
  ]
  coordinates {
  (0.0,68.71566666666669) +- (0.22150735640292704,0.22150735640292704)
  (0.03,76.15933333333336) +- (0.4689356678769995,0.4689356678769995)
  (0.06,82.83466666666656) +- (0.6154053154840816,0.6154053154840816)
  (0.09,90.493) +- (0.7987532559803928,0.7987532559803928)
  (0.12,98.43966666666674) +- (0.9515968185308391,0.9515968185308391)
  (0.15,107.07066666666654) +- (1.1168325935594843,1.1168325935594843)
  (0.18,116.17499999999998) +- (1.2404061778753157,1.2404061778753157)
  (0.21,124.09766666666661) +- (1.376226956577044,1.376226956577044)
  (0.24,133.84766666666667) +- (1.5236414318658087,1.5236414318658087)
  (0.27,144.6533333333334) +- (1.6995233726822354,1.6995233726822354)
  (0.3,152.45899999999978) +- (1.889183672513797,1.889183672513797)
  };

  \legend{Non Incremental,Incremental Baseline,Incremental RL}


  \end{axis}
  \end{tikzpicture}
	\end{minipage}
	\begin{minipage}{.47\textwidth}
		\begin{tikzpicture}[scale=0.8]
  \begin{axis}[
  xlabel={WER},
  ylabel={Mean task completion ratio},
  scaled ticks = false,
  tick label style={/pgf/number format/fixed},
  xmin=0, xmax=0.3,
  ymin=0.5, ymax=1,
  xtick={0,0.06,0.12,0.18,0.24,0.3},
  ytick={0.5,0.6,0.7,0.8,0.9,1},
  legend pos=south west,
  ymajorgrids=true,
  grid style=dashed,
  colorbrewer cycle list=Set1,
  ]

  \addplot+[
  error bars/.cd,
  y dir=both,
  y explicit,
  ]
  coordinates {
  (0.0,0.976999999999999) +- (0.005317306569642755,0.005317306569642755)
  (0.03,0.957666666666665) +- (0.007002288781376789,0.007002288781376789)
  (0.06,0.9459999999999981) +- (0.00794158848929817,0.00794158848929817)
  (0.09,0.929999999999998) +- (0.00905049440024744,0.00905049440024744)
  (0.12,0.9093333333333304) +- (0.009951619999892039,0.009951619999892039)
  (0.15,0.8776666666666643) +- (0.011656297952419533,0.011656297952419533)
  (0.18,0.8573333333333298) +- (0.012025877402041097,0.012025877402041097)
  (0.21,0.8209999999999981) +- (0.013776366435958311,0.013776366435958311)
  (0.24,0.7786666666666636) +- (0.01415042176992248,0.01415042176992248)
  (0.27,0.745666666666664) +- (0.015024266185076234,0.015024266185076234)
  (0.3,0.678333333333332) +- (0.016133166803548708,0.016133166803548708)
  };

  \addplot+[
  error bars/.cd,
  y dir=both,
  y explicit,
  ]
  coordinates {
  (0.0,0.9866666666666661) +- (0.004152651629448421,0.004152651629448421)
  (0.03,0.9766666666666655) +- (0.005351753191453632,0.005351753191453632)
  (0.06,0.9703333333333327) +- (0.006096653785297726,0.006096653785297726)
  (0.09,0.9496666666666649) +- (0.00756849374710839,0.00756849374710839)
  (0.12,0.932999999999998) +- (0.008924614640919192,0.008924614640919192)
  (0.15,0.901333333333331) +- (0.0108615364904491,0.0108615364904491)
  (0.18,0.8829999999999972) +- (0.01131227375906489,0.01131227375906489)
  (0.21,0.8443333333333309) +- (0.012983280263478267,0.012983280263478267)
  (0.24,0.8176666666666639) +- (0.01306325255397437,0.01306325255397437)
  (0.27,0.7793333333333299) +- (0.014605412427514938,0.014605412427514938)
  (0.3,0.725999999999997) +- (0.01487414888545534,0.01487414888545534)
  };

  \addplot+[
  error bars/.cd,
  y dir=both,
  y explicit,
  ]
  coordinates {
  (0.0,0.9879999999999994) +- (0.0038487932654281956,0.0038487932654281956)
  (0.03,0.9823333333333324) +- (0.004628579642948797,0.004628579642948797)
  (0.06,0.9719999999999989) +- (0.006091926081116591,0.006091926081116591)
  (0.09,0.9643333333333322) +- (0.006648320770441193,0.006648320770441193)
  (0.12,0.9443333333333317) +- (0.008136846356066953,0.008136846356066953)
  (0.15,0.9303333333333312) +- (0.00908389460284391,0.00908389460284391)
  (0.18,0.9159999999999974) +- (0.009918968548528275,0.009918968548528275)
  (0.21,0.888666666666664) +- (0.010941022044480372,0.010941022044480372)
  (0.24,0.8526666666666629) +- (0.012093480601824382,0.012093480601824382)
  (0.27,0.8229999999999964) +- (0.013342068065584875,0.013342068065584875)
  (0.3,0.7893333333333301) +- (0.013775514354584588,0.013775514354584588)
  };

  \legend{Non Incremental,Incremental Baseline,Incremental RL}

  \end{axis}
  \end{tikzpicture}
\end{minipage}
\caption{Mean dialogue duration and task for the non-incremental, the baseline incremental and the RL incremental strategies under different noise conditions.}
\label{fig:rlvsbaselines}
\end{figure*}
    
    	Three different strategies are compared:
        \begin{itemize}
        	\item \textbf{Non-incremental baseline:} It corresponds to the MixIni strategy defined in Chap. \ref{ch:baseline}. The user is asked to provide all the information necessary to execute her request and when there are still missing slots, the corresponding values are asked for one after another.
            \item \textbf{Incremental baseline:} MixIni+Incr from Chap. \ref{ch:baseline} is selected as an incremental baseline. It is identical to the non-incremental baseline with the difference that it is enhanced with handcrafted turn-taking rules defined in Chap. \ref{ch:baseline}.
            \item \textbf{Incremental RL:} It corresponds to the strategy learned with reinforcement learning.
        \end{itemize}
        
        Like in Chap. \cite{ch:baseline}, these strategies are compared under different levels of noise. The non-incremental and the incremental baselines have already been compared in Chap. \ref{ch:baseline}. In Fig. \ref{fig:rlvsbaselines}, they are also compared to the new automatically learned strategy. The differences become clearer as the WER increases. For WER=0.3, the non-incremental baseline reaches 3 minutes, the incremental baseline goes 10 seconds faster and the incremental RL still improves it by an additional 20 seconds (17\% gain in total). In terms of task completion, the non-incremental baseline drops under 70\%, the incremental baseline shows a performance of 73\% whereas the incremental RL keeps the ratio at a level of 80\%.

\chapter{Reinforcement learning for turn-taking optimisation}
\label{ch:rl}

	The experiments led in Chapter \ref{ch:baseline} showed that implementing incremental strategies in the Scheduler can improve dialogue efficiency. However, this approach requires the designer to handcraft the strategies most of the time in an empirical way. She has to come up with rules that are adapted to the type of the task at hand and to manually tune parameters. Moreover, the result is not guaranteed to be optimal.
	
	In this chapter, a new approach is proposed where the Scheduler automatically learns optimal turn-taking behaviours through interactions (published as \cite{Khouzaimi2015b}). Reinforcement learning is applied in order to make decisions at the micro-turn level based on a new state representation model. A new simulation experiment shows that the resulting strategy outperforms the handcrafted one from Chapter \ref{ch:baseline}.

\section{Reinforcement Learning Model}

        \subsection{Background}

             This section recalls a few elements about reinforcement learning and MDPs (see Chapter \ref{ch:soarl} for more details). The Scheduler will be modeled as an MDP: a tuple $(\mathcal{S},\mathcal{A},\mathscr{T},\mathscr{R},\gamma)$ where

             \begin{itemize}
                \item $\mathcal{S}$ is the state space: all the states in which the agent could be.
                \item $\mathcal{A}$ is the action space: all the actions that it can perform.
                \item $\mathscr{T}$ is the transition model: the distributions over the state space where the random variable is the next state $s' \in \mathcal{S}$ given that the agent is currently in state $s \in \mathcal{S}$ and performs action $a \in \mathcal{A}$.
                \item $\mathscr{R}$ is the reward model: the distributions over $\mathbb{R}$ where the random variable is the immediate reward $r \in \mathbb{R}$ given that the agent is currently at state $s \in \mathcal{S}$ and performs action $a \in \mathcal{A}$.
                \item $\gamma$ is a discount factor in $[0,1[$. The more it is close to 1, the more the agent is farsighted (maximising long term returns).
             \end{itemize}

             The time step at which the Scheduler is run is the micro-turn. Therefore, at each new micro-turn, the Scheduler computes the current state and then makes a decision (which action to perform). As discussed in Chapter \ref{ch:soarl}, the state space representation is a challenge in itself since it should embed all the necessary information while being tractable. In the next section, a new state representation adapted to incremental processing is introduced. It is general enough so that it can be easily implemented for any slot-filling task.

	\subsection{State representation}
    
    	At each new micro-turn $\mu T^{k,U}_i$, the following features are used to describe the system state (an interaction example where the features' values are given is provided rightafter in Table \ref{tab:dialexfeat}):
        
        \begin{itemize}
        	\item \textbf{SYSTEM\_REQ:} During the user dialogue turn $T^{k,U}$, the system is requiring a particular information. For instance, after an open question, it is waiting for all the slot values to be provided at once but it can also be waiting for a specific slot value or a response to a confirmation. This feature refers to the information that it is waiting for during $T^{k,U}$. It can take 6 different values\footnote{The dialogue act \textit{Done. Would you like to perform other actions?} is not a possible value since it is considered as a transition and not a part of the learning episode. For this specific dialogue turn, the Scheduler always choses to WAIT.} in the personal agenda management domain:
					
						\begin{itemize}
							\item General prompt: \textit{What can I do for you?}
							\item Description question: \textit{Please specify a description.}
							\item Date question: \textit{Please specify a date.}
							\item Time window question: \textit{Please specify a time window.}
							\item Description confirmation: \textit{You said <description> right?}
							\item Confirmation: \textit{Ok. So you want to add the event <event>. Is that right?}
						\end{itemize}
					
            \item \textbf{LAST\_INCR\_RESP:} As described in Chapter \ref{ch:architecture}, the Scheduler stores the last response it gets from the service at each micro-turn. It is used as a second feature which can take 11 different values, the 6 values possible for SYSTEM\_REQ and the five ones listed below:
						
						\begin{itemize}
							\item Conflict: \textit{Sorry. The time window <date> <time window> is already taken by the event <event>. What can I do for you?}
							\item Misunderstanding: \textit{Sorry I don't understand.}
							\item Yes or no misunderstanding: \textit{Sorry I don't understand. Please answer this question by saying yes or no.}
							\item Not existing event: \textit{Sorry, the event <event> does not exist. What can I do for you?}
							\item Other operations: \textit{Done. Would you like to perform other actions?}
						\end{itemize}
						
							The reason why this feature and SYSTEM\_REQ have different values possible even though they represent the same dialogue acts is that a single dialogue act can be viewed differently. For example, the system response \textit{Sorry. The time window <date> <time window> is already taken by the event <event>. What can I do for you?} is a conflict declaration from the LAST\_INCR\_RESP point of view but it is an open question when viewed as a SYSTEM\_REQ. Moreover, when the system declares a misunderstanding, SYSTEM\_REQ does not change (the system is still waiting for the same information). LAST\_INCR\_RESP represents the reaction that the system would have if interrupted at each micro-turn.
						
            \item \textbf{NB\_USER\_WORDS:} This feature is a counter of the number of words since the last change of LAST\_INCR\_RESP (the number of words since the Service did not change its mind about the response to deliver). It is equal to zero at the exact micro-turn when the change happens and it is incremented until the next change.
						
            \item \textbf{NORMALISED\_SCORE:} At each micro-turn, the ASR score is updated: most of the time, it is multiplied by the ASR score corresponding to the new incoming word (see Figure \ref{fig:asrsimu}). Except from the cases where a boost comes into play, the score keeps decreasing as the user speaks. To avoid penalising long sentences, the score is normalised by taking the geometric mean over the words (this induces a bias since the number of inputs that forms the current ASR hypothesis may not be exactly the number of words because of the Scrambler's additions and deletions). If $s$ is the current score for $n$ number of words, NORMALISED\_SCORE = $s^{\frac{1}{n}}$.
            \item \textbf{TIME:} Corresponds to the duration of the current episode in milliseconds.
        \end{itemize}
				
				\begin{sidewaystable}
					\fontsize{7}{9}\selectfont
					\centering
					\begin{tabular}{|c|l|c|c|c|c|c|c|}
						\hline
						Turn or & Utterance (with score for user) & SYSTEM\_REQ & LAST\_INCR\_RESP & NB\_USER\_WORDS & NORMALISED\_SCORE & TIME & Scheduler's \\
						micro-turn & & & & & & & action\\
						\hline
						$T^{1,S}$ & What can I do for you? & & & & & & \\
						\hline
						$\mu T^{1,U}_1$ & [0.94] I & Open question & Misunderstanding & 0 & 0,94 & 300 & WAIT \\
						$\mu T^{1,U}_2$ & [0.92] I would & Open question & Misunderstanding & 1 & 0,96 & 600 & WAIT \\
						$\mu T^{1,U}_3$ & [0.81] I would like & Open question & Misunderstanding & 2 & 0,93 & 900 & WAIT \\
						$\mu T^{1,U}_4$ & [0.79] I would like to & Open question & Misunderstanding & 3 & 0,94 & 1200 & WAIT \\
						$\mu T^{1,U}_5$ & [0.65] I would like to add & Open question & Date question & 0 & 0,92 & 1500 & WAIT \\
						$\mu T^{1,U}_6$ & [0.61] I would like to add the & Open question & Date question & 1 & 0,92 & 1800 & WAIT \\
						$\mu T^{1,U}_7$ & [0.52] I would like to add the event & Open question & Date question & 3 & 0,91 & 2100 & WAIT \\
						$\mu T^{1,U}_8$ & [0.50] I would like to add the event birthday & Open question & Date question & 4 & 0,92 & 2400 & WAIT \\
						$\mu T^{1,U}_9$ & [0.48] I would like to add the event birthday & Open question & Date question & 5 & 0,92 & 2700 & WAIT \\
						& party & & & & & & \\
						$\mu T^{1,U}_{10}$ & [0.47] I would like to add the event birthday & Open question & Date question & 6 & 0,93 & 3000 & WAIT \\
						& party on & & & & & & \\
						$\mu T^{1,U}_{11}$ & [0.45] I would like to add the event birthday & Open question & Date question & 7 & 0,93 & 3300 & WAIT \\
						& party on January & & & & & & \\
						$\mu T^{1,U}_{12}$ & [0.22] I would like to add the event birthday & Open question & Time window question & 0 & 0,88 & 3600 & WAIT \\
						& party on January 6$^{th}$ & & & & & & \\
						$\mu T^{1,U}_{13}$ & [0.17] I would like to add the event birthday & Open question & Time window question & 1 & 0,87 & 3900 & WAIT \\
						& party on January 6$^{th}$ from & & & & & & \\
						$\mu T^{1,U}_{14}$ & [0.15] I would like to add the event birthday & Open question & Time window question & 2 & 0,87 & 4200 & WAIT \\
						& party on January 6$^{th}$ from 8pm & & & & & & \\
						$\mu T^{1,U}_{15}$ & [0.15] I would like to add the event birthday & Open question & Time window question & 3 & 0,88 & 4500 & WAIT \\
						& party on January 6$^{th}$ from 8pm until & & & & & & \\
						$\mu T^{1,U}_{16}$ & [0.11] I would like to add the event birthday & Open question & Conflict & 0 & 0,87 & 4800 & SPEAK \\
						& party on January 6$^{th}$ from 8pm until 11pm & & & & & & \\
						\hline
						$T^{2,S}$ & Sorry. The time window on January $6^{th}$ & & & & & & \\
						& from 8pm until 11pm is already & & & & & & \\
						& taken by the event writing essay that & & & & & & \\
						& is planned on January $6^{th}$ from 8pm & & & & & & \\
						& until 12am. What can I do for you? & & & & & & \\
						\hline
					\end{tabular}
					\caption{Dialogue example with state representation features' values.}
					\label{tab:dialexfeat}
				\end{sidewaystable}

        It is noteworthy that the only domain-related features are SYSTEM\_REQ and \\ LAST\_INCR\_RESP, therefore, in order to transpose this method to another domain, one should only provide the list of alternatives for both features. This is due to the fact that, instead of directly analysing the current partial utterance at each micro-turn, only the response it generates once fed to the Service is taken into account, and this is proven here to be a sufficient information.
        
        %HK> Justifier le choix d'une représentation linéaire
        %HK> Il faut introduire Fitted-Q, dire qu'on a essayé autre chose (tabulaire, Q-Learning...).
        A linear model is used to represent the Q-function \cite{Sutton1998}. First, it has been noticed that 21 combinations between SYSTEM\_REQ and LAST\_INCR\_RESP are frequently visited (the others barely happen or not at all). Therefore, 21 features are defined $\delta_1$,...,$\delta_{21}$ where $\delta_i = 1$ if and only if the current state corresponds to the $i^{th}$ combination, and 0 otherwise. The rare combinations are not included in the model since they require maintaining heavier models with no real improvements over the simpler ones.
        
        %HK> Utiliser eqnarray
        The Q-function should clearly not be monotonous with respect to NB\_USER\_WORDS since the user should not be interrupted too soon nor too late. It should be maximal around some value, therefore, the Q-function should not be linear with respect to that feature. Instead, NB\_USER\_WORDS is represented by three RBF functions $\phi^{nw}_1$, $\phi^{nw}_2$ and $\phi^{nw}_3$ centered in 0, 5 and 10 with a standard deviation of 2, 3 and 3. In other words
        
				\begin{eqnarray}
					\phi^{nw}_i & = & \exp \left( \frac{(NB\_USER\_WORDS - \mu_i)^2}{2 \sigma_i^2} \right) \\
          \mu_1 & = & 0, \text{ } \mu_2 = 5, \text{ }, \mu_3 = 10 \nonumber \\
          \sigma_1 & = & 2, \text{ } \sigma_2 = 3, \text{ } \sigma_3 = 3 \nonumber
				\end{eqnarray}
            
      	These values are empirical but they give more flexibility for the model to approach the true Q-function. Similarly, NORMALISED\_SCORE is also represented using two RBF functions $\phi^{ns}_1$ and $\phi^{ns}_2$ centered in 0.25 and 0.75 and with a standard deviation of 0.3 for both.
        
        Finally, TIME is normalised so that it is near zero at the beginning of the episode and around 1 when the duration reaches 6 minutes (the maximum duration due to patience limit):
        
				\begin{eqnarray}
					T & = & sigmoid \left( \frac{TIME-180}{60} \right)
				\end{eqnarray}
            
       	There is no need to use RBFs for this last feature since the Q-function is supposed to be monotonous with respect to it. The longer the dialogue, the more likely the user is to hang up.
        
        Therefore, the dialogue state is represented by the following vector
        
				\begin{eqnarray}
					\Phi(s) & = & [1,\delta_1,\delta_2,\delta_3,\phi^{nw}_1,\phi^{nw}_2,\phi^{nw}_3,\phi^{ns}_1,\phi^{ns}_2,T]
				\end{eqnarray}
            
   	\subsection{Actions, rewards and episodes}
    
    	%HK> Décrire plus en détail et justifier.
    	The system can perform the action WAIT and the action SPEAK (see Chapter \ref{ch:baseline} for a description of these actions). The action REPEAT, which consists on repeating the last word of the last stable utterance (with no intention of interrupting the user), introduces more complexity to the system for the following reasons:
			
			\begin{itemize}
				\item When performing a feedback, the speaker is not interrupted (total overlap) and therefore, in the case of a right feedback (repeating the true word that the user uttered), there is no visible impact on the dialogue duration. As a consequence, in simulation mode, the optimal strategy would be to perform feedbacks all the time which is of course not acceptable in real dialogue.
				\item Determining whether the user actually agreed with the feedback content and continued his sentence or whether he spotted an error and tried to correct it is a complicated problem in itself. The FEEDBACK\_RAW strategy implementation in Chapter \ref{ch:baseline} used the simple assumption that if the user's next word after the feedback is not a \textit{no}, it means that the feedback content is confirmed. This is of course an approximation that is aimed to make a first assessment of the potential of the corresponding TTP but it is not always the case in real dialogue.
				\item The way user's might react to a feedback is not certain. They might interpret such a behaviour as an interruption which is not the desired effect.
			\end{itemize}
			
			As a consequence, the action REPEAT is not addressed here.
        
    	At each micro-turn, the system receives a reward $-\Delta t$ which corresponds to the opposite of the time elapsed since the micro-turn before. Moreover, there are two rewarding situations where the system gets a reward of 150 (the reward the Scheduler gets when an elementary task - see Chapter \ref{ch:strategies} - is completed):
        
        \begin{itemize}
        	\item The system takes the floor to confirm that the task corresponding to the user's requests has been accomplished. Happens when the user says \textit{yes} to a confirmation question like in the following dialogue:

                  \begin{dialogue}
                    \speak{SYSTEM} Ok. So you want to add the event birthday party on January 6$^{th}$ from 8pm until 11pm. Is that right?
                    \speak{USER} Yes.
                    \speak{SYSTEM} Ok, done. Is there anything else I can do for you?
                  \end{dialogue}

            \item The system takes the floor to declare a conflict, for example: \textit{Sorry. The time window on March $3^{rd}$ from 2pm until 5pm is not available since the event dentist is scheduled on March 3$^{rd}$ from 2pm until 3pm}. Even though the task has not been accomplished, the system has successfully done its job (all the information slots have been successfully gathered and a response has been computed, even though an incoherence is reported).
        \end{itemize}
        
        An episode is a portion of a dialogue that starts with an open question (where the user is supposed to utter a complete request with all the necessary slot values) and ends with either a new open question or a user hang up (open questions due to confirmation failures do not start a new episode).
            
	\subsection{Fitted-Q Value Iteration}
    
    	Fitted-Q iteration \cite{Bellman1959} has already been successfully applied to dialogue management in the traditional sense \cite{Chandramohan2010}. Here it is applied to the problem of turn-taking\footnote{Q-Learning has been tried at first but it learnt poorly (it needs very important amount of data to converge). This is coherent with previous literature \cite{Lemon2006}. In \cite{Daubigney2013}, Fitted-Q has been shown to perform better.}. Recall that the Bellman optimality equation states that
			
			\begin{eqnarray}
				Q^*(s,a) & = & \mathbb{E}[\mathscr{R} (s,a,s') + \gamma \max_{a'} Q^*(s,a') | s, a] \\
				Q^* & = & T^* Q^*
			\end{eqnarray}
			
				The operator $T^*$ is a contraction \cite{Bellman1957}. As a consequence, there is a way to estimate it in an iterative way: \textit{Value Iteration} (Banach theorem). Each new iteration is linked to the previous one as follows:
        
			\begin{eqnarray}
				Q_i & = & T^* Q_{i-1}
			\end{eqnarray}
						
					However, an exact representation of the Q-function is assumed which is not possible in the present case since the state space is infinite. Discretisation of continuous variables is generally a poor option since it implies arbitrary choices of granularity and phase. Also, it does not solve the curse of dimensionality (and may even make it worse). A classical solution consists in using a linear representation of the Q-function:
        
			\begin{eqnarray}
				\hat{Q}(s,a) & = & \theta(a)^T \Phi(s)
			\end{eqnarray}
            
      	where $\theta(a)$ is a parameter vector associated with action a. The aim of Fitted-Q algorithm is to estimate the parameters that approximate the Q-function $Q^*$ best. $\hat{Q}$ is the projection of Q on the space of the functions that can be written as a linear combination of the state vector's components. Let $\Pi$ be the corresponding projection operator, then it can be shown that $\Pi T^*$ is still a contraction and admits a unique fixed point that can be iteratively computed as follows: $\hat{Q}_{\theta_i (a)} = \Pi T^* \hat{Q}_{\theta_{i-1} (a)}$ for each action $a$. Since the transition probabilities of the MDP and the reward function are not known, a sampled operator $\hat{T}$ is used instead of $T$. For a transition $(s_j,a_j,r_j,s_j')$, it is defined as
        
				\begin{eqnarray}
					\hat{T} Q(s_j,a_j) & = & r_j + \gamma \max_{a'} Q(s_j',a')
				\end{eqnarray}
            
      	The Fitted-Q algorithm therefore estimates the $\theta$ vector using the iteration rule: $\hat{Q}_{\theta_i (a)} = \hat{T} \hat{Q}_{\theta_{i-1} (a)}$ for each action $a$. To compute the projection, the least square estimator is used:
        
				\begin{eqnarray}
					\theta_i (a) & = & \argmin_{\theta \in \mathbb{R}^p} \sum_{j=1}^N \left(r_j + \gamma \max_a [\theta_{i-1}(a)^T \phi(s_j')] - \theta (a_j)^T \phi(s_j) \right)^2
				\end{eqnarray}
            
      	where N is the number of transitions in the data batch. This is a classic least square optimisation and $\theta_i$ can be computed as follows (since the matrix inversion does not depend on $i$, it has to be performed only once):
        
				\begin{eqnarray}
					\theta_i (a) & = & \left( \sum_{j=1}^N \phi(s_j)^T \phi(s_j) \right)^{-1} \sum_{j=1}^N \phi(s_j) \left( r_j + \gamma \max_a \theta_{i-1}(a)^T \phi(s_j) \right)
				\end{eqnarray}

\section{Experiment}
	
    \subsection{Setup}

		The dialogue scenarios described in Chapter \ref{ch:baseline} are used here. During learning, the noise level is fixed at 0.15. 50 parallel runs have been produced with 3000 episodes each and the average curve is depicted in Figure \ref{fig:learn}. The $\theta$ parameters in the Q-function model are initiated to zeros and updated every 500 episodes, therefore, the learnt strategy reward evolution is representation by one average point in the middle of each 500 episodes interval. There are three phases to distinguish:
        
        %HK> Faire une référence au chapitre sur l'optimisation du travail sur le dilemme exploitation/exploration.
        \begin{enumerate}
        	\item \textbf{Pure exploration (Episodes 0-500):} The actions are taken randomly with a probability of 0.9 for choosing WAIT and 0.1 for SPEAK. Picking equiprobable actions results in the user being interrupted so often that the interesting scenarios are very rarely explored.
            \item \textbf{Exploration/exploitation (Episodes 500-2500):} An $\epsilon$-greedy policy is used with respect to the current Q-function, with $\epsilon=0.1$ (unlike the previous phase, when a random action is picked, the actions WAIT and SPEAK are equiprobable).
            \item \textbf{Pure exploitation (Episodes 2500-3000):} A 100\% greedy policy is used.
        \end{enumerate}
				
				\begin{figure}[htp]
      \centering
      \begin{tikzpicture}[scale=0.8]
      \begin{axis}[
      xlabel={Number of episodes},
      ylabel={Reward},
      scaled ticks = false,
      tick label style={/pgf/number format/fixed},
      xmin=100, xmax=3000,
      ymin=95, ymax=120,
      xtick={100,500,1000,1500,2000,2500,3000},
      ytick={95,100,105,110,115,120},
      legend pos=south east,
      ymajorgrids=true,
      grid style=dashed,
      ]

  \addplot[
	color=color3,
	thick, mark=triangle*, mark size=2.5,
  error bars/.cd,
  y dir=both,
  y explicit,
  ]
  coordinates {
  (100,104.5)
  (150,104.5)
  (200,104.5)
  (250,104.5)
  (300,104.5)
  (350,104.5)
  (400,104.5)
  (450,104.5)
  (500,104.5)
  (550,104.5)
  (600,104.5)
  (650,104.5)
  (700,104.5)
  (750,104.5)
  (800,104.5)
  (850,104.5)
  (900,104.5)
  (950,104.5)
  (1000,104.5)
  (1050,104.5)
  (1100,104.5)
  (1150,104.5)
  (1200,104.5)
  (1250,104.5)
  (1300,104.5)
  (1350,104.5)
  (1400,104.5)
  (1450,104.5)
  (1500,104.5)
  (1550,104.5)
  (1600,104.5)
  (1650,104.5)
  (1700,104.5)
  (1750,104.5)
  (1800,104.5)
  (1850,104.5)
  (1900,104.5)
  (1950,104.5)
  (2000,104.5)
  (2050,104.5)
  (2100,104.5)
  (2150,104.5)
  (2200,104.5)
  (2250,104.5)
  (2300,104.5)
  (2350,104.5)
  (2400,104.5)
  (2450,104.5)
  (2500,104.5)
  (2550,104.5)
  (2600,104.5)
  (2650,104.5)
  (2700,104.5)
  (2750,104.5)
  (2800,104.5)
  (2850,104.5)
  (2900,104.5)
  (2950,104.5)
  (3000,104.5)
  };

  \addplot[
	color=color5,
	thick, mark=*, mark options={fill=white},
  error bars/.cd,
  y dir=both,
  y explicit,
  ]
  coordinates {
  (100,110)
  (150,110)
  (200,110)
  (250,110)
  (300,110)
  (350,110)
  (400,110)
  (450,110)
  (500,110)
  (550,110)
  (600,110)
  (650,110)
  (700,110)
  (750,110)
  (800,110)
  (850,110)
  (900,110)
  (950,110)
  (1000,110)
  (1050,110)
  (1100,110)
  (1150,110)
  (1200,110)
  (1250,110)
  (1300,110)
  (1350,110)
  (1400,110)
  (1450,110)
  (1500,110)
  (1550,110)
  (1600,110)
  (1650,110)
  (1700,110)
  (1750,110)
  (1800,110)
  (1850,110)
  (1900,110)
  (1950,110)
  (2000,110)
  (2050,110)
  (2100,110)
  (2150,110)
  (2200,110)
  (2250,110)
  (2300,110)
  (2350,110)
  (2400,110)
  (2450,110)
  (2500,110)
  (2550,110)
  (2600,110)
  (2650,110)
  (2700,110)
  (2750,110)
  (2800,110)
  (2850,110)
  (2900,110)
  (2950,110)
  (3000,110)

  };

  %\addplot[
  %only marks,
  %error bars/.cd,
  %y dir=both,
  %y explicit,
  %]
  %coordinates {
  %(100,99.9554)
  %(150,100.126)
  %(200,100.1544)
  %(250,100.3698)
  %(300,100.1902)
  %(350,100.0802)
  %(400,99.6666)
  %(450,99.643)
  %(500,99.8262)
  %(600,111.0898)
  %(650,110.619)
  %(700,110.4986)
  %(750,110.667)
  %(800,110.6698)
  %(850,110.5932)
  %(900,110.6876)
  %(950,111.0216)
  %(1000,111.2774)
  %(1050,110.7884)
  %(1100,110.9832)
  %(1150,111.0464)
  %(1200,110.8506)
  %(1250,110.6576)
  %(1300,110.903)
  %(1350,111.7178)
  %(1400,111.428)
  %(1450,110.8136)
  %(1500,110.7896)
  %(1550,110.9168)
  %(1600,110.5906)
  %(1650,110.4182)
  %(1700,110.7948)
  %(1750,110.792)
  %(1800,110.6854)
  %(1850,110.485)
  %(1900,110.3116)
  %(1950,110.4132)
  %(2000,110.2364)
  %(2050,110.5788)
  %(2100,111.0018)
  %(2150,110.7796)
  %(2200,111.6892)
  %(2250,111.791)
  %(2300,110.1606)
  %(2350,110.5042)
  %(2400,111.7458)
  %(2450,111.0896)
  %(2500,110.744)
  %(2600,114.2912)
  %(2650,114.86)
  %(2700,114.6676)
  %(2750,114.1698)
  %(2800,114.3308)
  %(2850,114.8298)
  %(2900,114.7522)
  %(2950,114.0682)
  %(3000,113.6722)
  %};
	
	\addplot[
	color=color11,
	thick, mark=x, mark size=3, mark options={line width=2\pgflinewidth},
  error bars/.cd,
  y dir=both,
  y explicit,
  ]
  coordinates {
  (250,100.0013111)
  (750,110.7915556)
  (1250,111.0210889)
  (1750,110.5252444)
  (2250,111.0562)
  (2750,114.4046444)
  };

  \legend{Non Incremental,Incremental Baseline,Incremental RL}

  \end{axis}
  \end{tikzpicture}
  \caption{Learning curve (0-500: pure exploration, 500-2500: exploration/exploitation, 2500-3000: pure exploitation) with WER$=0.15$.}
  \label{fig:learn}
\end{figure}

\begin{figure*}[htp]
\centering
\begin{minipage}{.47\textwidth}
		\begin{tikzpicture}[scale=0.8]
    \begin{axis}[
    xlabel={WER},
    ylabel={Mean duration (sec)},
    scaled ticks = false,
    tick label style={/pgf/number format/fixed},
    xmin=0, xmax=0.3,
    ymin=60, ymax=240,
    xtick={0,0.06,0.12,0.18,0.24,0.3},
    ytick={60,90,120,150,180,210,240},
    legend pos=north west,
    ymajorgrids=true,
    grid style=dashed,
    ]

  \addplot[
	color=color3,
	thick, mark=triangle*, mark size=2.5,
  error bars/.cd,
  y dir=both,
  y explicit,
  ]
  coordinates {
  (0.0,81.81333333333323) +- (0.49131639706677,0.49131639706677)
  (0.03,91.71533333333338) +- (0.7370071114727688,0.7370071114727688)
  (0.06,100.61999999999999) +- (0.9163154073570945,0.9163154073570945)
  (0.09,110.30933333333337) +- (1.1079317542734202,1.1079317542734202)
  (0.12,120.25133333333329) +- (1.3145078817622518,1.3145078817622518)
  (0.15,128.88766666666666) +- (1.4520881830480932,1.4520881830480932)
  (0.18,139.9666666666667) +- (1.6146051787906812,1.6146051787906812)
  (0.21,148.08833333333314) +- (1.7406157103623126,1.7406157103623126)
  (0.24,160.28433333333334) +- (2.0239947579988615,2.0239947579988615)
  (0.27,170.64066666666687) +- (2.1548187185752323,2.1548187185752323)
  (0.3,179.7433333333333) +- (2.2832048422084354,2.2832048422084354)
  };

  \addplot[
	color=color5,
	thick, mark=*, mark options={fill=white},
  error bars/.cd,
  y dir=both,
  y explicit,
  ]
  coordinates {
  (0.0,72.33766666666655) +- (0.19309601399017187,0.19309601399017187)
  (0.03,81.02566666666667) +- (0.5634594051387006,0.5634594051387006)
  (0.06,88.5193333333334) +- (0.6941501827939834,0.6941501827939834)
  (0.09,97.77333333333321) +- (0.9424363565520608,0.9424363565520608)
  (0.12,106.84066666666676) +- (1.0881492743933865,1.0881492743933865)
  (0.15,116.92933333333318) +- (1.2450282893676852,1.2450282893676852)
  (0.18,125.67533333333333) +- (1.460507242474365,1.460507242474365)
  (0.21,137.1566666666666) +- (1.6699996326865956,1.6699996326865956)
  (0.24,145.81366666666656) +- (1.7355578985505493,1.7355578985505493)
  (0.27,156.351) +- (1.9864460266108706,1.9864460266108706)
  (0.3,168.65766666666676) +- (2.124651277704671,2.124651277704671)
  };

  \addplot[
	color=color11,
	thick, mark=x, mark size=3, mark options={line width=2\pgflinewidth},
  error bars/.cd,
  y dir=both,
  y explicit,
  ]
  coordinates {
  (0.0,68.71566666666669) +- (0.22150735640292704,0.22150735640292704)
  (0.03,76.15933333333336) +- (0.4689356678769995,0.4689356678769995)
  (0.06,82.83466666666656) +- (0.6154053154840816,0.6154053154840816)
  (0.09,90.493) +- (0.7987532559803928,0.7987532559803928)
  (0.12,98.43966666666674) +- (0.9515968185308391,0.9515968185308391)
  (0.15,107.07066666666654) +- (1.1168325935594843,1.1168325935594843)
  (0.18,116.17499999999998) +- (1.2404061778753157,1.2404061778753157)
  (0.21,124.09766666666661) +- (1.376226956577044,1.376226956577044)
  (0.24,133.84766666666667) +- (1.5236414318658087,1.5236414318658087)
  (0.27,144.6533333333334) +- (1.6995233726822354,1.6995233726822354)
  (0.3,152.45899999999978) +- (1.889183672513797,1.889183672513797)
  };

  \legend{Non Incremental,Incremental Baseline,Incremental RL}


  \end{axis}
  \end{tikzpicture}
	\end{minipage}
	\begin{minipage}{.47\textwidth}
		\begin{tikzpicture}[scale=0.8]
  \begin{axis}[
  xlabel={WER},
  ylabel={Mean task completion ratio},
  scaled ticks = false,
  tick label style={/pgf/number format/fixed},
  xmin=0, xmax=0.3,
  ymin=0.5, ymax=1,
  xtick={0,0.06,0.12,0.18,0.24,0.3},
  ytick={0.5,0.6,0.7,0.8,0.9,1},
  legend pos=south west,
  ymajorgrids=true,
  grid style=dashed,
  ]

  \addplot[
	color=color3,
	thick, mark=triangle*, mark size=2.5,
  error bars/.cd,
  y dir=both,
  y explicit,
  ]
  coordinates {
  (0.0,0.976999999999999) +- (0.005317306569642755,0.005317306569642755)
  (0.03,0.957666666666665) +- (0.007002288781376789,0.007002288781376789)
  (0.06,0.9459999999999981) +- (0.00794158848929817,0.00794158848929817)
  (0.09,0.929999999999998) +- (0.00905049440024744,0.00905049440024744)
  (0.12,0.9093333333333304) +- (0.009951619999892039,0.009951619999892039)
  (0.15,0.8776666666666643) +- (0.011656297952419533,0.011656297952419533)
  (0.18,0.8573333333333298) +- (0.012025877402041097,0.012025877402041097)
  (0.21,0.8209999999999981) +- (0.013776366435958311,0.013776366435958311)
  (0.24,0.7786666666666636) +- (0.01415042176992248,0.01415042176992248)
  (0.27,0.745666666666664) +- (0.015024266185076234,0.015024266185076234)
  (0.3,0.678333333333332) +- (0.016133166803548708,0.016133166803548708)
  };
p
  \addplot[
	color=color5,
	thick, mark=*, mark options={fill=white},
  error bars/.cd,
  y dir=both,
  y explicit,
  ]
  coordinates {
  (0.0,0.9866666666666661) +- (0.004152651629448421,0.004152651629448421)
  (0.03,0.9766666666666655) +- (0.005351753191453632,0.005351753191453632)
  (0.06,0.9703333333333327) +- (0.006096653785297726,0.006096653785297726)
  (0.09,0.9496666666666649) +- (0.00756849374710839,0.00756849374710839)
  (0.12,0.932999999999998) +- (0.008924614640919192,0.008924614640919192)
  (0.15,0.901333333333331) +- (0.0108615364904491,0.0108615364904491)
  (0.18,0.8829999999999972) +- (0.01131227375906489,0.01131227375906489)
  (0.21,0.8443333333333309) +- (0.012983280263478267,0.012983280263478267)
  (0.24,0.8176666666666639) +- (0.01306325255397437,0.01306325255397437)
  (0.27,0.7793333333333299) +- (0.014605412427514938,0.014605412427514938)
  (0.3,0.725999999999997) +- (0.01487414888545534,0.01487414888545534)
  };

  \addplot[
	color=color11,
	thick, mark=x, mark size=3, mark options={line width=2\pgflinewidth},
  error bars/.cd,
  y dir=both,
  y explicit,
  ]
  coordinates {
  (0.0,0.9879999999999994) +- (0.0038487932654281956,0.0038487932654281956)
  (0.03,0.9823333333333324) +- (0.004628579642948797,0.004628579642948797)
  (0.06,0.9719999999999989) +- (0.006091926081116591,0.006091926081116591)
  (0.09,0.9643333333333322) +- (0.006648320770441193,0.006648320770441193)
  (0.12,0.9443333333333317) +- (0.008136846356066953,0.008136846356066953)
  (0.15,0.9303333333333312) +- (0.00908389460284391,0.00908389460284391)
  (0.18,0.9159999999999974) +- (0.009918968548528275,0.009918968548528275)
  (0.21,0.888666666666664) +- (0.010941022044480372,0.010941022044480372)
  (0.24,0.8526666666666629) +- (0.012093480601824382,0.012093480601824382)
  (0.27,0.8229999999999964) +- (0.013342068065584875,0.013342068065584875)
  (0.3,0.7893333333333301) +- (0.013775514354584588,0.013775514354584588)
  };

  \legend{Non Incremental,Incremental Baseline,Incremental RL}

  \end{axis}
  \end{tikzpicture}
\end{minipage}
\caption{Mean dialogue duration and task for the non-incremental, the baseline incremental and the RL incremental (after convergence) strategies under different noise conditions.}
\label{fig:rlvsbaselines}
\end{figure*}
   
   	\subsection{Results}
    
    	Three different strategies are compared:
        \begin{itemize}
        	\item \textbf{Non-incremental baseline:} It corresponds to the MixIni strategy defined in Chapter \ref{ch:baseline}. The user is asked to provide all the information necessary to execute her request and when there are still missing slots, the corresponding values are asked for one after another.
            \item \textbf{Incremental baseline:} MixIni+Incr from Chapter \ref{ch:baseline} is selected as an incremental baseline. It is identical to the non-incremental baseline with the difference that it is enhanced with handcrafted turn-taking rules defined in Chapter \ref{ch:baseline}.
            \item \textbf{Incremental RL:} It corresponds to the turn-taking strategy learned with reinforcement learning on top of the MixIni strategy for dialogue management.
        \end{itemize}
        
        Like in Chapter \ref{ch:baseline}, these strategies are compared under different levels of noise. The non-incremental and the incremental baselines have already been compared in Chapter \ref{ch:baseline}. In Figure \ref{fig:rlvsbaselines}, they are also compared to the new automatically learnt strategy. The differences becomes clearer as the WER increases. For WER=0.3, the non-incremental baseline reaches 3 minutes, the incremental baseline goes 10 seconds faster and the incremental RL still improves it by an additional 20 seconds (17\% gain in total). In terms of task completion, the non-incremental baseline drops under 70\%, the incremental baseline shows a performance of 73\% whereas the incremental RL keeps the ratio at a level of 80\%.

        As a consequence, the reinforcement learning based strategy has been proven to improve the dialogue efficiency, even better than the handcrafted baseline. The proposed model was able to automatically learn optimal turn-taking decisions directly from interactions. In the next chapter, an experiment involving real users is run in order to validate these results.
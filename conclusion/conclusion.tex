\chapter*{Conclusion and future work}

	Viewed as a whole, the several contributions made in this thesis constitute a thorough methodology to enhance turn-taking capabilities of spoken dialogue systems. First, turn-taking mechanisms involved in human conversation are analysed which led to the establishment of a new turn-taking phenomena taxonomy. Compared to existing classifications in the literature, new analysis dimensions where the meaning behind the dialogue participants' behaviours, as well as the motivations behind them, are considered. This leads to a more fine-grained taxonomy which is relevant from the human-machine dialogue point of view. In addition, this constitutes the starting point from which the turn-taking phenomena that are the more likely to improve the dialogue efficiency are chosen.

        An incremental dialogue system can be built from scratch using an incremental version of each component in the dialogue chain. In this thesis, an alternative approach is proposed: a dialogue system is split into a Client and a Service part, then a new module, called the Scheduler, is inserted between the two. This new interface plays the role of a turn-taking manager and makes the set \{Scheduler+Service\} behaves like an incremental system from the Client's point of view. Two advantages are associated with this new approach: first, it makes it possible to transform an existing traditional dialogue system into an incremental one at a low cost and second, the traditional dialogue management part is clearly separated from the turn-taking management one and can be kept almost unchanged.

        Based on this new architecture, an incremental dialogue simulator, unique of its kind, has been implemented. It is able to generate dialogues in a personal agenda management domain. A User Simulator, coupled with an ASR Output Simulator that replicates ASR imprefections and instability, sends incremental requests to the Scheduler which decides when to take the floor to provide a response. A first simulation study where several slot-filling strategies along with two turn-taking strategies (non-incremental and handcrafted incremental) showed that the mixed-initiative one along with incremental processing achieves the best performance in terms of dialogue duration and task completion.

        Since handcrafting turn-taking strategies requires the designers to empirically set all the parameters and doing so for each new condition (task, language, etc.), this approach is not guaranteed to be optimal while requiring important labour and time resources. On the other hand, data-driven techniques make it possible to build optimal strategies at lower costs. In the field of human-machine dialogue, reinforcement learning has been proven to be particularly useful since no annotation effort is required (unlike supervised techniques) and it is able to learn from delayed rewards (thus making it possible to take the whole dialogue quality as a reward function). In this thesis, a new reinforcement learning turn-taking strategy is proposed and trained using the previous dialogue simulator. In simulation, it has been shown to reduce dialogue duration while improving the task completion ratio when compared to the non-incremental and the handcrafted incremental baselines.

        Finally, the previous strategies have been transposed to a new domain, the Marjordomo, then they have been tested through real users interactions. The handcrafted and the reinforcement learning strategies slightly reduce the dialogue duration, but the latter significantly improves the task completion ratio (by 15\% compared to the non-incremental strategy). Also, compared with the handcrafted strategy, the data-driven one takes more risk by deciding to speak before a user silence (hence significantly reducing the response latency) while maintaining a low false cut-in rate of 6.8\% instead of 31\%.

        To conclude, from a general point of view, this thesis provides new evidence showing the potential of incremental dialogue processing. More particularly, it goes a step further by showing that optimal turn-taking can be learnt automatically during the interactions. Also, the proposed architecture and the generality of the feature used for learning makes it possible to easily transfer this work to any domain and to bootstrap from existing dialogue systems.
        
        During the course of this thesis, two questions were also tackled but due to time constraints, they are still under investigation and there is still work to be done in order to hopefully come up with interesting results. These questions are:

        \begin{itemize}
          \item How to adapt the exploration/exploitation mechanism in reinforcement learning to the case of incremental processing?
          \item How to learn both optimal dialogue management and optimal turn-taking decisions at the same time?
        \end{itemize}

        \paragraph{Exploration/exploitation:} The exploration/exploitation dilemma in reinforcement learning is a research problem in itself. Inspired by the multi-armed bandit literature, the most simple and naive approaches are $\epsilon$-greedy and Boltzmann exploration (Softmax) \cite{Sutton1998} (some other simple approaches also exist in the literature but they are less used and they are based on similar ideas). A new approach (initially proposed to solve the multi-armed bandit problem) called Upper Confidence Bound (UCB) \cite{Auer2002} is also frequently used since it improves the convergence rate. This algorithm has also inspired other algorithms which are more adapted to reinforcement learning like UCRL \cite{Auer2006}. Nevertheless, incremental processing raises a new challenge which is not tackled in existing approaches: in this case, the optimal policy is not balanced when it comes to the number of times each action has to be chosen. The action which consists in doing nothing (called WAIT in this thesis) and wait for further information coming from the ASR should be picked the majority of the time. Therefore, incremental processing involves long episodes where the system should perform one action most of the time and rarely pick an alternative action. As a consequence, during the exploration process using existing methods, the agent rarely performs the right decisions at the right time. Therefore, there is a need to come up with a more adapted and more efficient way to deal with the exploration/exploitation dilemma in this case. In Chapter \ref{ch:rl}, an $\epsilon$-greedy policy with a bias in favour of the WAIT action has been proven to be successful in this case. However, there is no guarantee that this is the optimal way to proceed and more importantly, it is likely that this method would perform poorly if the micro-turn duration is increasing (imagine a setup with a new micro-turn every millisecond). To investigate this problem, two ideas have been proposed during this thesis:

        \begin{itemize}
          \item \textbf{SMDP-based approach:} Semi-Markov Decision Processes (SMDPs) \cite{Bradtke1994} are different from conventional MDPs in the sense that time is continuous and the time interval between two decisions is not constant (the instants in which the agent is asked to make decisions is defined by a separate process). This is an interesting framework when it comes to incremental dialogue processing and the idea is to cast the Scheduler as an SMDP that is asked to make decisions at specific instants (the rest of the time, it is always picking the WAIT action). Most of the time, when the WAIT action is picked, the state associated with the next micro-turn is very similar to the state associated with the current state. Therefore, the agent will be asked to make decisions only when the state varies in a significant way or in other words, when the state variation given a specific metric reaches a certain threshold. The investigated method tries to learn an optimal value for this threshold in an online fashion.
          \item \textbf{DPS-based approach:} Direct Policy Search (DPS) consists in using classical optimisation methods directly over the policy space unlike conventional reinforcement learning algorithms where a Q-function is computed and then the policy is derived from it. The advantage of such methods is that they naturally embed the mechanism described in the previous point: when two states are similar, a policy is most likely to associate the same action to both of them.
        \end{itemize}

        \paragraph{DM-Scheduler co-learning:} The approach depicted in Chapter \ref{ch:rl} assumes that the dialogue manager (in the Service) has a constant behaviour. However, a very active research thread proposed many methods where this module is constantly improving its decisions mostly using reinforcement learning. Considering that, a natural and legitimate question comes into play: is it possible for both the turn-taking manager and the conventional dialogue manager to simultaneously learn optimal behaviours from interactions. Such a study can benefit from the large existing literature about multi-agent collaborative learning \cite{Claus1998,Panait2005,Vogel2013}.

\chapter*{R\'esum\'e}

     Les syst\`emes de dialogue incr\'ementaux sont capables d'entamer le traitement des paroles de l'utilisateur au moment m\^eme o\`u il les prononce (sans attendre de signal de fin de phrase tel un long silence par exemple). Ils peuvent ainsi prendre la parole \`a n'importe quel moment et l'utilisateur peut faire de m\^eme (et interrompre le syst\`eme). De ce fait, ces syst\`emes permettent d'effectuer une plus large palette de comportements de prise de parole en comparaison avec les syst\`emes de dialogue traditionnels. Cette th\`ese s'articule autour de la probl\'ematique suivante : est-il possible pour un syst\`eme de dialogue incr\'emental d'apprendre une strat\'egie optimale de prise de parole de fa\c con autonome? Tout d'abord, une analyse des mechanismes sous-jacents \`a la dynamique de prise de parole dans une conversation homme-homme a permis d'\'etablir une taxonomie de ces ph\'enom\`enes. Ensuite, une nouvelle architecture permettant de doter les syst\`emes de dialogues conventionels de capacit\'es de traitement incr\'ementales de la parole, \`a moindre co\^ut, a \'et\'e propos\'ee. Dans un premier temps, un simulateur de dialogue destin\'e \`a r\'epliquer les comportements incr\'ementaux de l'utilisateur et de la reconnaissance vocale a \'et\'e d\'evelopp\'e puis utilis\'e pour effectuer les premier tests de strat\'egies de dialogue incr\'ementales. Ces derni\`eres ont \'et\'e d\'evelopp\'ees \`a base de r\`egles issues de l'analyse effectu\'ee lors de l'\'etablissement de la taxonomie des ph\'enom\`enes de prise de parole. Les r\'esultats de la simulation montrent que le caract\`ere incr\'emental permet d'obtenir des interactions plus efficaces. La meilleure strat\'egie \`a base de r\`egles a \'et\'e retenue comme r\'ef\'erence pour la suite.

     Dans un second temps, une strat\'egie bas\'ee sur l'apprentissage par renforcement a \'et\'e impl\'ement\'ee. Elle est capable d'apprendre \`a optimiser ses d\'ecisions de prise de parole de fa\c con totalement autonome \'etant donn\'ee une fonction de r\'ecompense. Une premi\`ere comparaison, en simulation, a montr\'e que cette strat\'egie engendre des r\'esultats encore meilleurs par rapport \`a la strat\'egie \`a base de r\`egles. En guise de validation, une exp\'erience avec des utilisateurs r\'eels a \'et\'e men\'ee (interactions avec une maison intelligente). Une am\'elioration significative du taux de completion de t\^ache a \'et\'e constat\'ee dans le cas de la strat\'egie apprise par renforcement et ce, sans d\'egradation de l'appreciation globale par les utilisateurs de la qualit\'e du dialogue (en r\'ealit\'e, une l\'eg\`ere am\'elioration a \'et\'e constat\'ee).

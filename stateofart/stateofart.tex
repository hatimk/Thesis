\chapter{State of the art}

\label{ch:stateofart}

\section{Human dialogue}
	\subsection{Philosophy of language}
    \label{soa:philolang}
    
    	%HK> Parler de Clark également : communicative vs. meta-communicative tracks.
    
    	If I say \textit{This dog is big}, I utter a few sounds that can be cast as words. How are these words related to the real objects I refer to? How comes that a sequence of sounds can have effects on others? I can give orders to somebody and make them performs the actions I want as I can congratulate or insult someone and have an emotional impact on her. Also, how comes an utterance can also be judged as a complete nonsense or as a true or false assertion? These are a few questions raised in the philosophy of language.
    
    	In his book called \textit{How To Do Things With Words} \cite{Austin1962}, J.L. Austin focuses on the concept of \textit{speech act} which the title of another book \cite{Searle1969} by John Searle, who extended this theory of language. Introducing this concepts is aimed towards bringing answers to the previous questions. Saying \textit{My sister just arrived}, one performs a speech act that can be viewed from different points of view. Suppose that someone is listening as this sentence is being uttered and that person does not speak English, then obviously she only hears a sequence of noises which is the physical, low-level nature of the speech act. When considered from that perspective, the latter is referred to as a \textit{locutionary act}. On the other hand, when the focus is the meaning of the utterance and the message the speakers wants to deliver, the speech act is an \textit{illocutionary act}. Finally, saying something to somebody can have psychological effects on that person: congratulating someone on insulting him car be rewarding or hurting, a strong grounded speech can be convincing...etc... These are referred to as \textit{perlocutionary acts}.
        
        To build a dialogue system, the traditional approach is to consider the user's and the system's speech acts as illocutionary and perlocutionary acts. When studying incremental dialogue systems, the \textit{locutionary act} point of view comes at play. In this thesis, this distinction will be clarified.
        
    \subsection{Turn-taking in human dialogue}
    \label{soa:ttphuman}
    
    	%HK> Citer Tanenhaus + article lecture incrémentale.
        %HK> Résultats Interspeech 2013 Jens Edlund
    
    	Turn-taking is a sociological phenomenon that is encountered in many different situations: card games, road traffic regulation, CPU resource sharing...etc... In \cite{Sacks1974}, Harvey Sacks describes the social organisation of turn-taking as an economy where \textit{turns are valued, sought or avoided}, depending on the situation at hand. Then, in the rest of his paper, he focuses on the case of human conversation. Obviously, this is subject to many contextual and cultural variations but the objectif of \cite{Sacks1974} is to meet the challenge of extracting a set of rules that would ultimately describe the human turn-taking mechanisms in a general fashion.
        
        For about six years, Sacks has been analysing conversation recordings and he came up with a few rules that caracterise human conversation and turn-taking. One of us is particularly interesting given the purpose of this thesis: \textit{transition (from one turn to a next) with no gap an no overlap are common. Together with transitions characterized by slight gap or slight overlap, they make up the vast majority of transitions}.
        
        In \cite{Beattie1982}, a political interview between Margaret Thatcher and Jim Callaghan has been analysed. As a result, the author introduced a classification of turn-taking phenomena where each category is caracterised by the answer to these three questions:
        
        \begin{enumerate}
            \item Is the attempted speaker switch successful?
            \item Is there simultaneous speech?
            \item Is the first speaker's utterance complete?
        \end{enumerate}
        
        %HK> Citer Baumann et Johnsduttir  après Meena...
        Optimising turn-taking means taking the floor at the right time. Humans are very good at detecting the cues for these timings. In artificial dialogue systems, different kinds of features can be used in order to detect these timings \cite{Gravano2011}: prosodic features, lexical features, semantic features...etc... and a lot of work has already been done in this direction \cite{Meena2013} 
        
        %HK> Parler de la méthode, 12 ans d'études basées sur des enregistrements...
        %HK> Parler des points propices au turn-taking, du principe de minimise gaps and overlaps, thread de recherche basé sur la prosodie.
        %HK> Autres classifications, Beatie1982 et dérivées...
        %HK> Duncan1972 a aussi le mérite d'être cité
    
\section{Spoken dialogue systems and incremental behaviour}
    
    	\subsubsection{Spoken dialogue system}
        \label{soa:sds}
        
        %HK> Citer papier SIGDIAL 2014 juste après companionship.
        	A spoken dialogue system (SDS) is an automated application that interacts directly with a human being in natural language. Virtual assistant like Siri (Apple) or Cortana (Microsoft) are good examples of SDSs. They are task-oriented as their goal is to help to user to achieve some task. There also exist a few SDSs that are only used for chatting and companionship.
            
            \begin{figure*}[ht]
		      	\centering
      			\includegraphics[scale=0.8]{stateofart/DialogueChain.pdf}
	      		\caption{Simulated environment architecture}
      			\label{fig:dialchain}
			\end{figure*}
        
            The classic architecture of an SDS is made of five main modules (Figure \ref{fig:dialchain}):
            \begin{enumerate}
            	\item Automatic Speech Recognition (ASR): transforms the user's audio speech signal into text.
                \item Natural Language Understanding (NLU): outputs a conceptual representation of the user's utterance in text format.
                \item Dialogue Manager (DM): given the concepts extracted from the user's request, a response (in a conceptual format too) is computed.
                \item Natural Language Generation (NLG): transforms the concepts computed by the DM into text.
                \item Text-To-Speech (TTS): reads the text outputted by the NLG by using a synthetic voice.
            \end{enumerate}

            %HK> Rajouter des références.
            Speech recognition technology is an old problem with long history. During the 1950s, a group of researchers from Bell Labs developed the first technology that is able to recognise digits from speech (in fact, speech perception has been under study since the early 1930s). Then, during the second half of the last century, new advances have made it possible to build ASR solutions with larger vocabulary and with no dependence on the user. In the 1960s, Hidden Markov Models (HMMs) were proved to be useful for speech recognition and they were the most popular technique two decades later. Comercial products using ASR technology had to wait until the 1990s to be finally released in the marked as they reached and interesting vocabulary (even though their accuracy and their delay were far behind the technology we have today). Performances kept improving incrementally until 2009, when Deep Learning algorithms were tested; the Word Error Rate (WER) decreased by 30\%. During the last six years, research continued in that direction giving birth to accurate and reactive speech recognition solutions (Google, Nuance, Sphinx, Kaldi...). Therefore, ASR is no longer a bottleneck in the development of spoken dialogue systems, and as we will see later, the delays they offer make it possible to design reactive incremental dialogue systems. Commercial off-the-shelf ASR solutions like Google ASR or Nuance products are able to recognise almost every word in many languages, including named entities. Finally, the ASR output is not only the text that the recognition algorithm figures out to be the best match for the input audio signal, but a list of the N most likely hypotheses and the corresonding confidence scores: it is called the N-Best. For instance, an 5-Best could be:

            \begin{itemize}
              \item 0.965: I would like to book a flight from New-York to Los Angeles
              \item 0.931: I could like to book a flight for New-York to Los Angeles
              \item 0.722: Cold like a book fly New-York to Los Angeles
              \item 0.570: Cold like look flight pork to Los Angeles
              \item 0.441: Cold bike look fly New-York to Los Angeles  
            \end{itemize}

            %HK> Référence sur Rule-based vs. statistical-based? Un survey sur la NLU?
            The scope of NLU technology is a subfield of Natural Language Processing (NLP) whose scope much wider than the spoken dialogue field. Since the 1950s, researchers have been trying to develop several models and ontologies in order to automatically process natural language with several applications in sight: topic recognition, sentiment analysis, news filtering and analysis, natural speech processing...etc...The ambition manifested during 1950s and the early 1960s quickly had to face reality as the expected objectives were far from being reached. As a consequence, research in this area was significantly slowed down between the 1960s and the 1980s. During the last decade, NLP reasearch has found a second wind thanks to new Machine Learning, bringing them at the heart of lucrative businesses like recommendation and digital marketing. NLU refers to the set of techniques in order to make the machine understand the underlying structure of a text in natural language. To do so, a lexicon as well as an ontology (concepts and the links between them in a specific domain) should be built. Existing dialogue systems are able to interact with the user in the domain they are built for only, however, during the last few years, researchers have been pushing the boundaries of open-domain systems \cite{Gasic2013}. Therefore, earlier NLU solutions are based on a set of handcrafted parsing rules, however, new statistical-based models have been proven to be more robust and easy to generalise over domains.

            %HK> Faire un choix: "reinforcement learning", "Reinforcement Learning" ou "RL". Idem "spoken dialogue systems", "Spoken Dialogue Systems" ou bien "SDSs".
            %HK> Autres références pour le DSTC, RNN de Hendersen...etc...
            Dialogue Management is at the heart of Spoken Dialogue System's current research. A couple of decades ago, for the first time, dialogue has been modeled as Markov Decision Processes (MDPs) problem, hence being solved using reinforcement learning \cite{Eckert1997}. The dialogue state is generally defined by the whole dialogue history and the set of actions in each state are the dialogue acts that the system can make. In 2007, in order to represent the uncertainty over the user's intent (due to ASR or NLU imperfections), dialogues have been cast as Partially Observable Markov Decision Processes (POMDPs) \cite{Williams2007}. This gave raise to the notion of \textit{believe tracking} which objective is to keep a distribution over the possible user intents. In order to encourage research in that direction, a Dialogue State Tracking Challenge (DSTC) has been launched a few years ago \cite{Williams2012b}, bringing new contributions each year. This approach has been shown to provide intersting results, however, the size and the dimensionality of the state space makes it untractable most of the time.

            %HK> Noms de startups et de boites faisant de la génération de texte automatique?
            The NLG task is the inverse of the NLU one. It started being used in the 1990s for purposes like financial news summary. A few startups and big companies also provide automatic text generation solutions that are used to quickly produce reports or official letters. The main challenge for such systems is to be able to generate a variety of different words, expressions and sentencs in order for them not to be repetitive and for the result to be as realistic as possible. This is even more crucial when it comes to dialogue systems as they are supposed to simulate a real conversation with the user which is a highly variable task).

            During the 1930s, Bell Labs were not only interested in ASR but they also developed a new approach for the inverse task: the TTS (also known as \textit{speech synthesis}). The human speech is broken into small acoustic components that are sequentially pronounced by the system. They built the first machine demonstrating this mechanism: the Voder. As far as this task is concerned, the challenge for the system si to sound as human-like as possible, in terms of phoneme transitions, speech rate and prosody. Even though substantial advances have been accomplished since the Voder, it is still easy to distinguish between a synthetised and a real human voice.

            %HK> Parler du test de Turing comme test ultime?

        \subsubsection{Incremental dialogue systems}
    
        	The idea of incremental systems goes back to incremental compilers \cite{Lock1965}. An incremental compile processes each new instruction independently from the previous ones. Therefore, a local modification of the code does not affect the whole result of the compilation. The idea of processing natural language in an incremental way is first introduced in \cite{Wiren1992}. Instead of feeding modules with complete utterances, the input is pushed chunk by chunk (500ms of audio signal, one word of a sentence...etc...) making the output change several times before the end of the user's utterance.

	    	Currently deployed dialogue systems have a simple and rigid way of managing turn-taking. The interaction mode they offer is similar to a walkie-talkie conversation as the system waits for the user to finish her utterance before taking the floor and vice-versa (even though some systems allow the user to interrupt the system). Such systems will be referred to as \textit{traditional dialogue systems} in this thesis.
        
        	%HK> Retrouver le papier sur la lecture et l'incrémental et le citer avec Tanenhaus.
        	%HK> Rajouter Beatie1982 dans le bibtex et le citer dans ce paragraphe juste après barging-in. Retrouver le papier qui a repris Beatie1982 et le citer avec lui.
    	    In human to human conversation, the listener does not wait for the speaker to finish his sentence before processing it; it processes it as it is spoken \cite{Tanenhaus1995}. As a consequence, human beings perform a large panel of turn-taking behaviours while speaking, like backchanneling (\textit{aha}, \textit{yeah}...) or barging-in.
            
            To replicate this behaviour, a new generation of SDSs has been the focus of current research for a few years. An SDS is said to be incremental when it is able to process the user's speech on the fly. The input signal is divided into small chunks and the growing sentence is reprocessed at each new chunk \cite{Schlangen2011}.

            \subsubsection{Advantages of incremental processing}

            \subsubsection{New challenges raised by incremental dialogue processing}
    
                %HK> Citer Dan Povey pour Kaldi.
                The first problem to consider when talking about incremental spoken dialogue systems is the question of ASR \textit{latency}, which is the time needed by the recognition algorithm to provide the text output corresponding to an audio signal. As we said earlier, the ASR accuracy has been a bottleneck in the development of spoken dialogue systems for many years but thanks to recent advances in this field, it is no longer the case. Similarly, incremental dialogue systems require quick responses from the ASR and the delays needed but speech recognition modules have been too long for many years which was a limiting factor in the development of incrmental dialogue systems. However, in the last few years, ASR technology has became reactive enough \cite{Platek2014}. Nevertheless, it is important to be aware that there is a tradeoff between the accuracy and the vocabulary size on one hand and the latency on the other hand. Kaldi, which is an ASR solution designed by researchers (and which is mostly used by them), makes it possible to design one's own acoustic and language model as well to set one's own parameters in order to control this tradeoff. On the other hand, more off-the-shelf solutions like Google ASR do not give the user such possibilities (however, accuracy and delays are well balanced for most applications).

                %HK> Citer Ethan et Jason au sujet de l'instabilité si ce n'est déjà fait.
                If we observe the successive partial results of an incremental ASR module during a user's utterance, it is likely that the progression they follow is not monotonous. In other words, a partial result is not guaranteed to be a prefix of results to come. The following example showing successive ASR results illustrate this phenomenon:

                \begin{enumerate}
                  \item Euh
                  \item I
                  \item Euh good
                  \item iPod
                  \item I would
                  \item I good bike
                  \item I would like
                \end{enumerate}

                This phenomenon is called ASR \textit{instability} (or stability depending on the sources) \cite{Selfridge2011}. This factor is also related to the tradeoff between latency and accuracy as prefering fast ASR over accurate ones can lead to very instable results (the system is not given enough time to check that its results are accurate, thus ending up delivering wrong partial results most of the time), and the vice versa.
								
		This leads to one of the main challenges raised by incremental processing: the ability to \textit{revise} the current hypothesis. All the modules in the dialogue chain are impacted by this problem. As an illustration, suppose that the user interacts with an incremental personal assistant on her phone and makes the following request \textit{Please call the number 01 45 80 12 35}. The last digit is first understood as being 30 and then 35, therefore, if the system is too reactive, there is a risk that it starts calling the wrong number and maybe start uttering the sentence \textit{Ok, calling 01 45 80 12 30}. Afterwards, the system understands 35 instead of 30 hence needing a correction mechanism in order to stop the TTS, to cancel the call, to perform a new one and to provide a new answer to the user. Nevertheless, even though the system at hand is equiped with such a mechanism, using it very often is not an optimal way of managing incremental input as it causes extra delay as well as non-natural behaviour (stopping the TTS and starting again with another utterance). This introduces a similar tradeoff to the one discussed for the ASR module but from the DM perspective: if decisions are taken too quickly, it is likely that some of them are wrong hence activating the correction mechanism. On the other hand, if the DM is slow to take action, then it lacks reactivity and there is no point for it to be incremental. As a consequence, it is important to determine the right moment to commit to the current partial utterance and to take action based on it \cite{Raux2008,Lu2011}. This constitues the main objective of this thesis.

                Incremental NLG also raise new problems which are illustrated in \cite{Baumann2013}. In this paper, a system has to describe a car's trajectory in a virtual world. When the latter approaches an intersection where it has to turn right or left (no road straight ahead), then the system utters something like \textit{The car drives along Main Street and then turns...euh...and then turns right}. In this example, the system is sure that the car is going to turn which makes it possible for it to commit to the first part of the sentence with no risk. However, this is not always the case as a new chunk of information from the user can change the whole system's response. In this thesis, the NLG is not incremental as we consider that the DM's response is computed instantly at each new micro-turn (event though it is not necessarily stabe and it can vary from micro-turn to micro-turn). Finally, in purely vocal applications, computing the NLG results incremental does not make much sense as the user's and the system's utterances do not overlap most of the time \cite{Sacks1974}. However, this is an interesting behaviour as far as multimodal applications are concerned.

                %HK> Trouver des références pour ce paragraphe et bien le justifier.
                Building an incremental TTS module can also be very tricky. In order for the synthetic voice to be the most human-like as possible, prosody should be computed accurately and to do so, the sentence's structure and punctuation have to be taken into account. This information is no longer given in the case of incremental TTS or it arrives too late.
		

    \subsection{Existing architectures}
    	\subsubsection{Sequential paradigm}
        
            A general abstract model of incremental dialogue systems has been introduced in \cite{Schlangen2011}. In this approach, the dialogue chain is maintained and each one of the five components is transformed into its incremental version. We will refer to this view of incremental dialogue systems as the \textit{sequential paradigm}.
            
            Each module is composed of three parts, the Left Buffer (LB), the Internal State (IS) and the Right Buffer(RB). As described in Section \ref{soa:sds}, each module is also characterised by the type of input it processes as well as the type of output it computes. In incremental dialogue, all these data flows have to be divided into small chunks which are called Incremental Units (IU) \cite{Schlangen2011}. For example, the audio signal that is given as an input to the ASR module can be divided into 500ms chunks that are processed one by one. Each IU is first added to the LB, then it is taken by IS for processing and once a result is available, a new IU of a new kind is outputted in the RB. As the RB of one module if the LB of the following one in the dialogue chain, the data propagation through the dialogue system is insured.
            
            It is important to note that a new IU in the LB does not necessarily imply that a new IU will be pushed into the RB on top of the ones that already existed there. An example given in \cite{Schlangen2011} is the following: suppose the user utters the number \textit{forty} which processed incrementally, then first the ASR outputs \textit{four} and then \textit{forty}. As a consequence, the second hypothesis does not complete the first one but it cancels it. This phenomenon will be discussed more in details in Chapter \ref{ch:simulation}.
            
            Adopting this paradigm is a natural way of enhancing traditional dialogue systems with incremental capabilities. It is interesting from a computational and design point of view as the different tasks are separated. Therefore, one is able to evaluate the different components independently \cite{Baumann2011} and have a global view on which area still needs improvement.
        
        \subsubsection{Multi-layer paradigm}
        
        	The problem of dialogue management in traditional dialogue systems can be formulated as follows: at each dialogue turn, given the dialogue context (including the last user's utterance), what is the right dialogue act to perform? In the incremental frame, this definition no longer holds as dialogue acts are no longer attached to dialogue turns. Therefore, one way to tackle the problem is to split the dialogue management task in two components, the high-level and the low-level handlers. This paradigm is directly motivated by Austin's, Searl's and Clark's contributions discussed in Section \ref{soa:philolang} as the high-level module handles illocutionary acts (the communicative track) whereas the low-level one manages locutionary acts (the meta-communicative track).
            
            %HK> Citer Pickering and Gravano 2003?
            As reported in \cite{Lemon2003}, this approach is more in alignment with results in the psycholinguistic field. The phenomena observed in the meta-communicative track are complex, and the interaction happen on multiple levels, not always following the classical dialogue chain. Having a separate module for handling these phenomena is therefore a more natural paradigm.
            
            Switching from the traditional dialogue management approach to the incremental one is also a transition from discrete time to continuous time, from a synchronous to an asynchronous processing \cite{Raux2007}. The low-level module is continuously (approximated by a high frequency processing in computers) listening to the outside world and waiting for events that might be interesting to communicate to the high-level handler. On the other hand, the latter communicates actions (dialogue acts) and it is the role of to the low-level module to choose whether to retrieve them to the user or not as well as choosing the right moment in case it decides to speak.
            
            Finally, starting from a traditional dialogue system, it is more easier and straightforward to transform it into an incremental one if one adopts this paradigm. Adding an extra low-level module to the dialogue manager is enough \cite{Selfridge2012a,Khouzaimi2014a}. At each new incremental input, this module sends the whole partial utterance from the beginning of the current turn to the dialogue manager and gets its response. Based on that and eventually some other features, it decides whether to take the floor or not. As most of the requests sent to the dialogue manager are "fake" as they are not meant to be acted on, they should not affect the dialogue context. Therefore, whether multiple instances of the dialogue manager are used, whether the dialogue context is saved and restored at each new request, unless the low-level module decides to take the floor.
    
\section{Reinforcement Learning}
	\subsection{Definition}
    
    	%HK> Insérer une image représentant l'interaction entre l'agent et l'environnement et s'y référer dans ce paragraphe.
        %HK> Remplacer tous les $$ par des eqnarray.
    	Reinforcement Learning (RL) is a sub-field of machine learning where an agent is put into an environment to interact with, and figure out through the process of \textit{trial and error}, what the best actions to take are, given a reward function to maximise \cite{Sutton1998} (see Fig. \ref{fig:rlscheme}).
        
        \begin{figure*}[ht]
          \centering
          \includegraphics[scale=0.8]{figures/RL.png}
          \caption{The interaction between the agent and the environment in reinforcement learning}
          \label{fig:rlscheme}
        \end{figure*}
        
        Formally, the agent is cast as a Markov Decision Process (MDP) which is a quintuple $\mathcal{M} = (\mathcal{S},\mathcal{A},T,R,\gamma)$ where:
        \begin{itemize}
        	\item $\mathcal{S}$ is the \textit{state space}. At each time step $t$, the agent is in some state $s_t \in \mathcal{S}$.
            \item $\mathcal{A}$ is the \textit{action space}. At each time step $t$, the agent decides to take action $a_t \in \mathcal{A}$.
            \item $T$ is the \textit{transition model}. It is the set of probabilities $\mathbb{P} (s_{t+1} = s'|s_t = s)$ for every $(s,s') \in \mathcal{S}^2$.
            \item $R$ is the \textit{reward model}. If $r_t$ is the immediate reward due to taking action $a_t$ in state $s_t$, then $R$ is the set of distributions of $r_t$ for every $(s_t,a_t) \in (\mathcal{S},\mathcal{A})$.
            \item $\gamma \in [0,1[$ is referred to as the \textit{discount factor}. In the RL framework, the aim of the agent is not to maximise the immediate reward but the \textit{expected return}, where the return $R_t$ is defined as follows:
            
            $$ R_t = r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + ... = \sum_{k=0}^\infty \gamma^k r_{t+k+1} $$
            
            Therefore, when $\gamma = 0$, the agent maximises the immediate reward only and when $\gamma$ tends towards 1, the agent maximises the sum of all the future rewards. More generally, the parameter $\gamma$ controls how far-sighted is the agent in terms of future rewards.
		\end{itemize}
        
        A \textit{policy} $\pi : \mathcal{S} \rightarrow \mathcal{A}$ is a mapping between the state space and the action space. An agent is said to follow the policy $\pi$ when for each time $t$, it takes the action $a_t = \pi(s_t)$. A policy can also be stochastic, in which case, $\pi (s,a)$ denotes the probability of choosing action a when the agent is at state s. A key aspect of MDPs is the \textit{Markov property}. Being at state $s$ is the only information necessary to predict the future expected return, and no information about the past is necessary. Therefore, given a policy, each state $s \in \mathcal{S}$ is given a value $V^\pi (s)$ which is the expected return for being at this state and following the policy $\pi$ afterwards:
        
        	$$ V^{\pi} (s) = \mathbb{E} [R_t | s_t = s, \pi] $$
            
      	Another interesting quantity is the expected return knowing the current state but also the next action, after which $\pi$ is followed. This is referred to as the Q-function:
        
        	$$ Q^{\pi} (s,a) = \mathbb{E} [R_t | s_t = s, a_t = a, \pi] $$
            
       	Given the definition of $R_t$, we can notice that 
        \begin{eqnarray}
          	V^{\pi} (s)   & = & \mathbb{E} [R_t | s_t = s, \pi] \nonumber \\
           	& = & \mathbb{E} [r_t + \sum_{k=0}^\infty \gamma^k r_{(t+1)+k+1} | s_t = s, \pi] \nonumber \\
            & = & \mathbb{E} [r_t + \gamma R_{t+1} | s_t = s, \pi] \nonumber \\
            & = & \mathbb{E} [r_t + \gamma V^{\pi} (s_{t+1}) | s_t = s, \pi] \nonumber
        \end{eqnarray}
            
     	This is known as the Bellman equation and it can also be written for the Q-function as follows
        
        	$$ Q^{\pi} (s_t,a_t) = \mathbb{E} [r_t + \gamma V^{\pi} (s_{t+1}) | s_t = s, a_t = a, \pi] $$
            
       	A natural question that can be asked at this point is: how do we compute these values? In reinforcement learning, this is known as the \textit{evaluation problem}. The transition model $T$ and the reward model $R$ are the elements that define the dynamic of the MDP. If they are known, $V^{\pi}$ can be directly computed. If we call $P_{ss'}^a = \mathbb{P} (s_{t+1} = s' | s_t = s, a_t = a)$ and $R_{ss'}^a$ the reward of chosing action a on state s and landing on s', we can write:
        
        \begin{eqnarray}
          	V^{\pi} (s)   & = & \mathbb{E} [R_t | s_t = s, \pi] \nonumber \\
           	& = & \sum_{a \in \mathcal{A}} \pi (s,a) \mathbb{E} [R_t | s_t = s, a_t = a, \pi] \nonumber \\
            & = & \sum_{a \in \mathcal{A}} \pi (s,a) \mathbb{E} [r_t + \gamma R_{t+1} | s_t = s, a_t = a, \pi] \nonumber \\
            & = & \sum_{a \in \mathcal{A}} \pi (s,a)  \sum_{s' \in \mathcal{S}} P_{ss'}^a (R_{ss'}^a + \gamma \mathbb{E} [R_{t+1} | s_{t+1} = s', \pi]) \nonumber \\
            & = & \sum_{a \in \mathcal{A}} \pi (s,a)  \sum_{s' \in \mathcal{S}} P_{ss'}^a (R_{ss'}^a + \gamma V^{\pi} (s')) \nonumber
        \end{eqnarray}
            
      	It is possible to define an order over the policies. Saying that $\pi_1$ is better that $\pi_2$ means that for all the states $s$, $V^{\pi_1} (s) \geq V^{\pi_2} (s)$. It can be shown that there exists at least one policy that is better that all the others: it is called the \textit{optimal policy} ($\pi^*$). To simplify the notations, $V^{\pi^*}$ will be referred to as $V^*$ and it defined as
        
        	$$ \forall s \in \mathcal{S}, \text{ } V^* (s) = \max_\pi V^\pi (s) $$
            
       	Similarly, we can define $Q^*$ as
        
        	$$ \forall (s,a) \in \mathcal{S}x\mathcal{A}, \text{ } Q^*(s,a) = \max_\pi Q^{\pi}(s,a) $$

        The aim of reinforcement learning is to learn the optimal policy. Similarly to what has been shown for $V^{\pi}$, if the transition and the reward models are known, the Bellman equation corresponding to $V^*$ can be written with respect to these models:

                $$ V^*(s) = \max_a \sum_{s' \in \mathcal{S}} P_{ss'}^a (R_{ss'}^a + \gamma V^*(s')) $$
        
        A similar form can be also be shown about the Q-function

                $$ Q^*(s,a) = \sum_{s' \in \mathcal{S}} P_{ss'}^a (R_{ss'}^a + \gamma \max_{a' \in \mathcal{A}} Q^*(s',a'))  $$

        A set of \textit{Dynamic Programming} methods exist in order to efficiently solve these kinds of equations and come up with the optimal policy given the transition and the reward model (knowing $Q^*$ implies knowing $\pi^*$ as the latter is the greedy policy with respect to the former Q-function). However, even though this kind of approaches are theoretically interesting, they only have a few practical applications as most of the times, $T$ and $R$ are unknown.

        Fortunately, there are other methods that have been shown to be successful in learning the optimal policy without any prior knowledge about the MDP dynamics as they completely learn by trial and error. The first set of approaches are called \textit{Monte Carlo}; the Q-function is directly evaluated by calculating the mean of the returns $R_t$ for each coupe $(s,a)$. Nevertheless, this method requires the current learning episode to be finished before updating the Q-function. Moreover, acting as such means evaluating $V^{\pi}$ (or $Q^{\pi}$ according to the chosen approach) with $\pi$ being the policy currently followed by the agent. So, how do we compute the optimal policy? This raises the problem of \textit{control} wich goes along with the question of evaluation introduced earlier. In fact, as we keep changing the Q-function values because of new agent experience, we also keep upating the policy to make it greedy with respect to the new evaluation results. Fig. \ref{fig:evalctrl} illustrates this idea.

        \begin{figure}
          \centering
          \includegraphics[scale=0.7]{figures/evalctrl.png}
          \caption{The alternation between evaluation and control}
          \label{fig:evalctrl}
        \end{figure}

        
    \subsection{Application to dialogue systems}
    
    	Reinforcement learning has been first applied to dialogue systems in \cite{Levin1997} and since then, it has the leading machine learning framework in the field. The dialogue state at time $t$ is generally determined by the history of dialogue acts since the beginning of the dialogue. At each turn, the set of actions is made of all the possible answer at that time. Partially Observable Markov Decision Processes (POMDPs) are also widely used. In this framework, the dialogue state is replaced by a distribution over all possible states which is a more natural way of modeling uncertainty, however, they are more complex and more difficult to scale \cite{Lemon2007}.
        
        % Citer un maximum de papiers après Meena2013, notamment Turn-Yielding cues qui a cité Beatie1982.
        In the field of incremental dialogue and turn-taking management, supervised learning is more common. The main problem tackled by researchers is the identification of the exact moments where the system should take the floor in order to achieve smooth turn-taking \cite{Raux2008,Meena2013}. Binary classifiers are used and the features they are fed are of different natures: lexical, semantic, prosodic...etc...However, a few papers tackled this problem by using reinforcement learning.
        
        \cite{Jonsdottir2008} used reinforcement learning while considering prosodic features only. Backchanneling for example can be performed by humans independently from the meaning. The cost function (negative reward) is taken as gaps and overlaps, hence following Sack's principle discussed in Section \ref{soa:ttphuman}.
        
        \cite{Dethlefs2012} adopted a complementary approach where only the semantic content of the user's utterance is taken into account (hierarchical reinforcement learning is used). In human conversation, it is more likely for the listener to react right after a relevant information. Similarly, in the case of a restaurant finding spoken dialogue system, the system should react right after understanding the restaurant's type or price range. In this work, the information pertinence is measured by the Information Density (ID). Therefore, the more the ID is high during system actions, the more reward it gets.
        
        Instead of trying to minimise gaps and overlaps, the reward function can be designed in a way to optimise dialogue duration and task completion like it is the case in \cite{Selfridge2010}. The system in this paper learns optimal initial turn-taking, in the sense that when a silence is detected, the dialogue participant that has the most relevant thing to say takes the floor first. Like in the previous paper, only semantic features are considered.
        
        A third approach to optimise turn-taking in spoken dialogue systems is to directly try to imitate human behaviours. In \cite{Kim2014} Inverse Reinforcement Learning is used to infer a reward function directly from user trajectories in a gathered dialogue corpus. Therefore, the reward function automatically incorporates objective and subjective dialogue quality criteria. The authors have made the choice not to consider lexical and semantic features, but rather to limit their work to timing and prosody signals.
    
    \subsection{Dialogue simulation}
    
    	A couple of decades ago, as dialogue systems started to become a popular research fields, the need for evaluation means in order to assess their quality started getting more and more important. Therefore, researchers turn to user simulation methods (also referred to as user modeling). In \cite{Eckert1997}, some of the advantages of these techniques are depicted: reduced cost with automatic evaluation of a large number of dialogues, less error risk, easy modeling of different user populations, possibility of using the same user model across different concurrent dialogue systems for comparison and providing a tool to quickly generate corpora for machine learning techniques at a low cost. Nevertheless, the authors recognise that user simulation cannot totally replace interactions with real users in the process of designing reliable dialogue systems: \textit{However, we believe that tests with human users are still vital for verifying the simulation models.}.
        
       	Simulating users accurately is a challenging task as their behaviours vary considerably from a person to another and moreover, the same user can change her preferences over time (concept-drift) \cite{Schatzmann2006}. Evaluating a user simulator and whether it handles such variability or not is a research track in itself \cite{Pietquin2013} and the qualities required are of different kinds. The trained user simulator should be consistent with the data that has been used for the training and the sequence of dialogue acts generated should be coherent. In addition, when it is used in turn to train a data-driven dialogue strategy, the quality of the latter is also an evaluation criteria. Also, it is important that the results obtained in simulation give strong indications about the behaviours with real users while being task independent and automatically able to automatically compute assessments.
        
        User simulation is useful during the conception phase of a dialogue system. However, training the simulator from data needs the dialogue system to be conceived already. Therefore, trying to come up with a simple model with only a few parameters is not always a bad idea as it has been proven to achieve good results as well \cite{Schatzmann2007}.
        
        %HK> Citer paradise après la liste des KPIs pour la reward function
        User simulator is also quite similar to the dialogue management task. As a consequence, it is legitimate to ask the following question: why not use reinforcement learning to train user simulators? The answer is that in the case of dialogue management, it is easier to come up with a reasonable reward function: task completion, dialogue duration, subjective evaluation...etc... When it comes to user simulation, the objective function is how well a real user is imitated which is impossible to handcraft. Fortunately, there exists a framework where the reward function is automatically inferred from data which is particularly useful here: inverse reinforcement learning \cite{Chandramohan2011}.
        
        When it comes to incremental dialogue systems, the only existing user simulator in our knowledge is the one described in \cite{Selfridge2012b}. Its state is update every 10 ms. However, the \textit{ASR instability} phenomenon is not replicated, that is to say that the ASR hypothesis construction is monotonous whereas in reality, it is not the case. When a new audio signal increment is heard by the ASR, the output can be partially or totally modified. In this simulator, only the simple case where a new increment is added to the output is modeled.

\section{Issues and motivation}
    
    A study led by the Market Intelligence and Consulting Institute shows that the market of artificial intelligence should grow exponentially in the next decade. Virtual assistants play the main role in this domain and they are predicted to multiply their market share by 2.5. Moreover, the market is expected to multiply its turn over by 14 (30\% growth per year on average). This shows that SDSs have reached a level of maturity that enables them to significantly interest the market. Therefore, designing robust, reliable and user-friendly spoken dialogue systems raises an important issue.
    
    Nevertheless, even though virtual assistants like Siri or other more domain specific spoken dialogue systems can offer a quite reasonable quality of service for certain tasks, the quality of the interaction is still poor, making room for improvement in different areas:
    
    % HK> Rajouter une citation pour les erreurs d'ASR et la gestion des erreurs.
    \begin{itemize}
    	\item The ASR module is not perfect and the errors that it engenders are not well handled by the DM.
        \item The vocabulary is very restrained hence users often use off-domain words, especially those who are not familiar with the system.
        \item Current Natural Language Processing (NLP) techniques still do not cover a lot of discourse formulations, therefore NLU modules are often too simple to handle all the situations encountered in real dialogue.
        \item Current SDSs do not adapt to the user's profile neither to groups of users's particular behaviours (accent, culture, specific expressions...).
        \item Turn-taking is handled in a walkie-talkie manner which is too simple compared to the reality of dialogue.
    \end{itemize}
    
    In this thesis we focus on the last point: turn-taking capabilities improvement. We identified two research streams:
    
    %HK> Revoir les papiers de références ici et bien les placer.
    \begin{enumerate}
    	\item Barge-in points identification to achieve smoother and more human like turn-taking. When a barge-in point is detected, it can be interpreted as an end-point so that the system can take the floor in a more reactive way, or as a suitable point for backchanneling. In these kind of studies, prosodic features are crucial but they can also be mixed with lexical and semantic ones.
        \item Turn-taking optimisation to improve dialogue efficiency (generally measured through dialogue duration and task completion). The main objective is to improve error handling by reporting errors quickly, and to improve reactivity in general as the system can respond as soon as it has enough information to do so. Unlike the previous stream, semantic features have more importance here.
    \end{enumerate}
    
    This thesis is a contribution to the second research stream. First we try to understand what is turn-taking in human-human interaction, then we ask ourselves what phenomena can be replicated in human-machine dialogue in order to offer a better error-handling and an increased efficiency in general.
    
    
    
    
    
    


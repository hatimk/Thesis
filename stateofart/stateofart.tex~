\chapter{State of the art}

\label{ch:stateofart}

\section{Human dialogue}
	\subsection{Philosophy of language}
    \label{soa:philolang}
    
    	%HK> Parler de Clark également : communicative vs. meta-communicative tracks.
    
    	If I say \textit{This dog is big}, I utter a few sounds that can be cast as words. How are these words related to the real objects I refer to? How comes that a sequence of sounds can have effects on others? I can give orders to somebody and make them performs the actions I want as I can congratulate or insult someone and have an emotional impact on her. Also, how comes an utterance can also be judged as a complete nonsense or as a true or false assertion? These are a few questions raised in the philosophy of language.
    
    	In his book called \textit{How To Do Things With Words} \cite{Austin1962}, J.L. Austin focuses on the concept of \textit{speech act} which the title of another book \cite{Searle1969} by John Searle, who extended this theory of language. Introducing this concepts is aimed towards bringing answers to the previous questions. Saying \textit{My sister just arrived}, one performs a speech act that can be viewed from different points of view. Suppose that someone is listening as this sentence is being uttered and that person does not speak English, then obviously she only hears a sequence of noises which is the physical, low-level nature of the speech act. When considered from that perspective, the latter is referred to as a \textit{locutionary act}. On the other hand, when the focus is the meaning of the utterance and the message the speakers wants to deliver, the speech act is an \textit{illocutionary act}. Finally, saying something to somebody can have psychological effects on that person: congratulating someone on insulting him car be rewarding or hurting, a strong grounded speech can be convincing...etc... These are referred to as \textit{perlocutionary acts}.
        
        To build a dialogue system, the traditional approach is to consider the user's and the system's speech acts as illocutionary and perlocutionary acts. When studying incremental dialogue systems, the \textit{locutionary act} point of view comes at play. In this thesis, this distinction will be clarified.
        
    \subsection{Turn-taking in human dialogue}
    \label{soa:ttphuman}
    
    	%HK> Citer Tanenhaus + article lecture incrémentale.
        %HK> Résultats Interspeech 2013 Jens Edlund
    
    	Turn-taking is a sociological phenomenon that is encountered in many different situations: card games, road traffic regulation, CPU resource sharing...etc... In \cite{Sacks1974}, Harvey Sacks describes the social organisation of turn-taking as an economy where \textit{turns are valued, sought or avoided}, depending on the situation at hand. Then, in the rest of his paper, he focuses on the case of human conversation. Obviously, this is subject to many contextual and cultural variations but the objectif of \cite{Sacks1974} is to meet the challenge of extracting a set of rules that would ultimately describe the human turn-taking mechanisms in a general fashion.
        
        For about six years, Sacks has been analysing conversation recordings and he came up with a few rules that caracterise human conversation and turn-taking. One of us is particularly interesting given the purpose of this thesis: \textit{transition (from one turn to a next) with no gap an no overlap are common. Together with transitions characterized by slight gap or slight overlap, they make up the vast majority of transitions}.
        
        In \cite{Beattie1982}, a political interview between Margaret Thatcher and Jim Callaghan has been analysed. As a result, the author introduced a classification of turn-taking phenomena where each category is caracterised by the answer to these three questions:
        
        \begin{enumerate}
        	\item Is the attempted speaker switch successful?
            \item Is there simultaneous speech?
            \item Is the first speaker's utterance complete?
        \end{enumerate}
        
        %HK> Citer Baumann et Johnsduttir  après Meena...
        Optimising turn-taking means taking the floor at the right time. Humans are very good at detecting the cues for these timings. In artificial dialogue systems, different kinds of features can be used in order to detect these timings \cite{Gravano2011}: prosodic features, lexical features, semantic features...etc... and a lot of work has already been done in this direction \cite{Meena2013} 
        
        %HK> Parler de la méthode, 12 ans d'études basées sur des enregistrements...
        %HK> Parler des points propices au turn-taking, du principe de minimise gaps and overlaps, thread de recherche basé sur la prosodie.
        %HK> Autres classifications, Beatie1982 et dérivées...
        %HK> Duncan1972 a aussi le mérite d'être cité
    
\section{Incremental Dialogue systems}
	\subsection{Definitions}
    
    	\subsubsection{Spoken dialogue system}
        \label{soa:sds}
        
        	The idea of incremental systems goes back to incremental compilers \cite{Lock1965}. An incremental compile processes each new instruction independently from the previous ones. Therefore, a local modification of the code does not affect the whole result of the compilation. The idea of processing natural language in an incremental way is first introduced in \cite{Wiren1992}. Instead of feeding modules with complete utterances, the input is pushed chunk by chunk (500ms of audio signal, one word of a sentence...etc...) making the output change several times before the end of the user's utterance.
        
        %HK> Citer papier SIGDIAL 2014 juste après companionship.
        	A spoken dialogue system (SDS) is an automated application that interacts directly with a human being in natural language without the intervention of a third person. Virtual assistant like Siri (Apple) or Cortana (Microsoft) are good examples of SDSs. They are task-oriented as their goal is to help to user to achieve some task. There also exist a few SDSs that are only used for chatting and companionship.
            
            \begin{figure*}[ht]
		      	\centering
      			\includegraphics[scale=0.8]{stateofart/DialogueChain.pdf}
	      		\caption{Simulated environment architecture}
      			\label{fig:dialchain}
			\end{figure*}
        
            The classic architecture of an SDS is made of five main modules (Figure \ref{fig:dialchain}):
            \begin{enumerate}
            	\item Automatic Speech Recognition (ASR): transforms the user's audio speech signal into text.
                \item Natural Language Understanding (NLU): outputs a conceptual representation of the user's utterance in text format.
                \item Dialogue Manager (DM): given the concepts extracted from the user's request, a response (in a conceptual format too) is computed.
                \item Natural Language Generation (NLG): transforms the concepts computed by the DM into text.
                \item Text-To-Speech (TTS): reads the text outputted by the NLG by using a synthetic voice.
            \end{enumerate}
        
        \subsubsection{Incremental dialogue systems}
    
	    	Currently deployed dialogue systems have a simple and rigid way of managing turn-taking. The interaction mode they offer is similar to a walkie-talkie conversation as the system waits for the user to finish her utterance before taking the floor and vice-versa (even though some systems allow the user to interrupt the system). Such systems will be referred to as \textit{traditional dialogue systems} in this thesis.
        
        	%HK> Retrouver le papier sur la lecture et l'incrémental et le citer avec Tanenhaus.
        	%HK> Rajouter Beatie1982 dans le bibtex et le citer dans ce paragraphe juste après barging-in. Retrouver le papier qui a repris Beatie1982 et le citer avec lui.
    	    In human to human conversation, the listener does not wait for the speaker to finish his sentence before processing it; it processes it as it is spoken \cite{Tanenhaus1995}. As a consequence, human beings perform a large panel of turn-taking behaviours while speaking, like backchanneling (\textit{aha}, \textit{yeah}...) or barging-in.
            
            To replicate this behaviour, a new generation of SDSs has been the focus of current research for a few years. An SDS is said to be incremental when it is able to process the user's speech on the fly. The input signal is divided in small chunks and the growing sentence is reprocessed at each new chunk \cite{Schlangen2011}.
    
    \subsection{Existing architectures}
    	\subsubsection{Sequential paradigm}
        
        	A general abstract model of incremental dialogue systems has been introduced in \cite{Schlangen2011}. In this approach, the dialogue chain is maintained and each one of the five components is transformed into its incremental version. We will refer to this view of incremental dialogue systems as the \textit{sequential paradigm}.
            
            Each module is composed of three parts, the Left Buffer (LB), the Internal State (IS) and the Right Buffer(RB). As described in Section \ref{soa:sds}, each module is also characterised by the type of input it processes as well as the type of output it computes. In incremental dialogue, all these data flows have to be divided into small chunks which are called Incremental Units (IU) \cite{Schlangen2011}. For example, the audio signal that is given as an input to the ASR module can be divided into 500ms chunks that are processed one by one. Each IU is first added to the LB, then it is taken by IS for processing and once a result is available, a new IU of a new kind is outputted in the RB. As the RB of one module if the LB of the following one in the dialogue chain, the data propagation through the dialogue system is insured.
            
            It is important to note that a new IU in the LB does not necessarily imply that a new IU will be pushed into the RB on top of the ones that already existed there. An example given in \cite{Schlangen2011} is the following: suppose the user utters the number \textit{forty} which processed incrementally, then first the ASR outputs \textit{four} and then \textit{forty}. As a consequence, the second hypothesis does not complete the first one but it cancels it. This phenomenon will be discussed more in details in Chapter \ref{ch:simulation}.
            
            Adopting this paradigm is a natural way of enhancing traditional dialogue systems with incremental capabilities. It is interesting from a computational and design point of view as the different tasks are separated. Therefore, one is able to evaluate the different components independently \cite{Baumann2011} and have a global view on which area still needs improvement.
        
        \subsubsection{Multi-layer paradigm}
        
        	The problem of dialogue management in traditional dialogue systems can be formulated as follows: at each dialogue turn, given the dialogue context (including the last user's utterance), what is the right dialogue act to perform? In the incremental frame, this definition no longer holds as dialogue acts are no longer attached to dialogue turns. Therefore, one way to tackle the problem is to split the dialogue management task in two components, the high-level and the low-level handlers. This paradigm is directly motivated by Austin's, Searl's and Clark's contributions discussed in Section \ref{soa:philolang} as the high-level module handles illocutionary acts (the communicative track) whereas the low-level one manages locutionary acts (the meta-communicative track).
            
            %HK> Citer Pickering and Gravano 2003?
            As reported in \cite{Lemon2003}, this approach is more in alignment with results in the psycholinguistic field. The phenomena observed in the meta-communicative track are complex, and the interaction happen on multiple levels, not always following the classical dialogue chain. Having a separate module for handling these phenomena is therefore a more natural paradigm.
            
            Switching from the traditional dialogue management approach to the incremental one is also a transition from discrete time to continuous time, from a synchronous to an asynchronous processing \cite{Raux2007}. The low-level module is continuously (approximated by a high frequency processing in computers) listening to the outside world and waiting for events that might be interesting to communicate to the high-level handler. On the other hand, the latter communicates actions (dialogue acts) and it is the role of to the low-level module to choose whether to retrieve them to the user or not as well as choosing the right moment in case it decides to speak.
            
            Finally, starting from a traditional dialogue system, it is more easier and straightforward to transform it into an incremental one if one adopts this paradigm. Adding an extra low-level module to the dialogue manager is enough \cite{Selfridge2012a,Khouzaimi2014a}. At each new incremental input, this module sends the whole partial utterance from the beginning of the current turn to the dialogue manager and gets its response. Based on that and eventually some other features, it decides whether to take the floor or not. As most of the requests sent to the dialogue manager are "fake" as they are not meant to be acted on, they should not affect the dialogue context. Therefore, whether multiple instances of the dialogue manager are used, whether the dialogue context is saved and restored at each new request, unless the low-level module decides to take the floor.
    
\section{Reinforcement Learning}
	\subsection{Definition}
    
    	%HK> Insérer une image représentant l'interaction entre l'agent et l'environnement et s'y référer dans ce paragraphe.
        %HK> Remplacer tous les $$ par des eqnarray.
    	Reinforcement Learning (RL) is a sub-field of machine learning where an agent is put into an environment to interact with, and figure out through the process of \textit{trial and error}, what the best actions to take are, given a reward function to maximise \cite{Sutton1998}.
        
        Formally, the agent is cast as a Markov Decision Process (MDP) which is a quintuple $\mathcal{M} = (\mathcal{S},\mathcal{A},T,R,\gamma)$ where:
        \begin{itemize}
        	\item $\mathcal{S}$ is the \textit{state space}. At each time step $t$, the agent is in some state $s_t \in \mathcal{S}$.
            \item $\mathcal{A}$ is the \textit{action space}. At each time step $t$, the agent decides to take action $a_t \in \mathcal{A}$.
            \item $T$ is the \textit{transition model}. It is the set of probabilities $\mathbb{P} (s_{t+1} = s'|s_t = s)$ for every $(s,s') \in \mathcal{S}^2$.
            \item $R$ is the \textit{reward model}. If $r_t$ is the immediate reward due to taking action $a_t$ in state $s_t$, then $R$ is the set of distributions of $r_t$ for every $(s_t,a_t) \in (\mathcal{S},\mathcal{A})$.
            \item $\gamma \in [0,1[$ is referred to as the \textit{discount factor}. In the RL framework, the aim of the agent is not to maximise the immediate reward but the \textit{expected return}, where the return $R_t$ is defined as follows:
            
            $$ R_t = r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + ... = \sum_{k=0}^\infty \gamma^k r_{t+k+1} $$
            
            Therefore, when $\gamma = 0$, the agent maximises the immediate reward only and when $\gamma$ tends towards 1, the agent maximises the sum of all the future rewards. More generally, the parameter $\gamma$ controls how far-sighted is the agent in terms of future rewards.
		\end{itemize}
        
        A \textit{policy} $\pi : \mathcal{S} \rightarrow \mathcal{A}$ is a mapping between the state space and the action space. An agent is said to follow the policy $\pi$ when for each time $t$, it takes the action $a_t = \pi(s_t)$. A key aspect of MDPs is the \textit{Markov property}. Being at state $s$ is the only information necessary to predict the future expected return, and no information about the past is necessary. Therefore, given a policy, each state $s \in \mathcal{S}$ is given a value $V^\pi (s)$ which is the expected return for being at this state and following the policy $\pi$ afterwards:
        
        	$$ V^{\pi} (s) = \mathbb{E} [R_t | s_t = s, \pi] $$
            
      	Another interesting quantity is the expected return knowing the current state but also the next action, after which $\pi$ is followed. This is referred to as the Q-function:
        
        	$$ Q^{\pi} (s,a) = \mathbb{E} [R_t | s_t = s, a_t = a, \pi] $$
            
     	Taking the Markov property into account, the value of state $s_t$ can be viewed as the immediate reward plus $\gamma$ times the value at time $s_{t+1}$. That is what the Bellman equation says:
        
        	$$ V^{\pi} (s_t) = r_t + \gamma V^{\pi} (s_{t+1})$$
            
      	It also holds for the Q-function:
        
        	$$ Q^{\pi} (s_t,a_t) = r_t + \gamma Q^{\pi} (s_{t+1},\pi{s_{t+1}}) $$
            
      	The goal of reinforcement learning is to find the policy that maximises the value function over all states. This is called the \textit{optimal policy} $\pi^*$, hence, for all $s \in \mathcal{S}$, $V^{\pi^*} (s) \geq V^{\pi} (s)$. It can be shown that
        
        	$$ V^{\pi^*} (s_t) = r_t + \gamma  $$
            
      	
            
    	%HK> Revoir les équations de Bellman, les écrire en intégrant T et R (Dynamic Programming Approach)
        
    \subsection{Application to dialogue systems}
    
    	Reinforcement learning has been first applied to dialogue systems in \cite{Levin1997} and since then, it has the leading machine learning framework in the field. The dialogue state at time $t$ is generally determined by the history of dialogue acts since the beginning of the dialogue. At each turn, the set of actions is made of all the possible answer at that time. Partially Observable Markov Decision Processes (POMDPs) are also widely used. In this framework, the dialogue state is replaced by a distribution over all possible states which is a more natural way of modeling uncertainty, however, they are more complex and more difficult to scale \cite{Lemon2007}.
        
        % Citer un maximum de papiers après Meena2013, notamment Turn-Yielding cues qui a cité Beatie1982.
        In the field of incremental dialogue and turn-taking management, supervised learning is more common. The main problem tackled by researchers is the identification of the exact moments where the system should take the floor in order to achieve smooth turn-taking \cite{Raux2008,Meena2013}. Binary classifiers are used and the features they are fed are of different natures: lexical, semantic, prosodic...etc...However, a few papers tackled this problem by using reinforcement learning.
        
        \cite{Jonsdottir2008} used reinforcement learning while considering prosodic features only. Backchanneling for example can be performed by humans independently from the meaning. The cost function (negative reward) is taken as gaps and overlaps, hence following Sack's principle discussed in Section \ref{soa:ttphuman}.
        
        \cite{Dethlefs2012} adopted a complementary approach where only the semantic content of the user's utterance is taken into account (hierarchical reinforcement learning is used). In human conversation, it is more likely for the listener to react right after a relevant information. Similarly, in the case of a restaurant finding spoken dialogue system, the system should react right after understanding the restaurant's type or price range. In this work, the information pertinence is measured by the Information Density (ID). Therefore, the more the ID is high during system actions, the more reward it gets.
        
        Instead of trying to minimise gaps and overlaps, the reward function can be designed in a way to optimise dialogue duration and task completion like it is the case in \cite{Selfridge2010}. The system in this paper learns optimal initial turn-taking, in the sense that when a silence is detected, the dialogue participant that has the most relevant thing to say takes the floor first. Like in the previous paper, only semantic features are considered.
        
        A third approach to optimise turn-taking in spoken dialogue systems is to directly try to imitate human behaviours. In \cite{Kim2014} Inverse Reinforcement Learning is used to infer a reward function directly from user trajectories in a gathered dialogue corpus. Therefore, the reward function automatically incorporates objective and subjective dialogue quality criteria. The authors have made the choice not to consider lexical and semantic features, but rather to limit their work to timing and prosody signals.
    
    \subsection{Dialogue simulation}
    
    	A couple of decades ago, as dialogue systems started to become a popular research fields, the need for evaluation means in order to assess their quality started getting more and more important. Therefore, researchers turn to user simulation methods (also referred to as user modeling). In \cite{Eckert1997}, some of the advantages of these techniques are depicted: reduced cost with automatic evaluation of a large number of dialogues, less error risk, easy modeling of different user populations, possibility of using the same user model across different concurrent dialogue systems for comparison and providing a tool to quickly generate corpora for machine learning techniques at a low cost. Nevertheless, the authors recognise that user simulation cannot totally replace interactions with real users in the process of designing reliable dialogue systems: \textit{However, we believe that tests with human users are still vital for verifying the simulation models.}.
        
       	Simulating users accurately is a challenging task as their behaviours vary considerably from a person to another and moreover, the same user can change her preferences over time (concept-drift) \cite{Schatzmann2006}. Evaluating a user simulator and whether it handles such variability or not is a research track in itself \cite{Pietquin2013} and the qualities required are of different kinds. The trained user simulator should be consistent with the data that has been used for the training and the sequence of dialogue acts generated should be coherent. In addition, when it is used in turn to train a data-driven dialogue strategy, the quality of the latter is also an evaluation criteria. Also, it is important that the results obtained in simulation give strong indications about the behaviours with real users while being task independent and automatically able to automatically compute assessments.
        
        User simulation is useful during the conception phase of a dialogue system. However, training the simulator from data needs the dialogue system to be conceived already. Therefore, trying to come up with a simple model with only a few parameters is not always a bad idea as it has been proven to achieve good results as well \cite{Schatzmann2007}.
        
        %HK> Citer paradise après la liste des KPIs pour la reward function
        User simulator is also quite similar to the dialogue management task. As a consequence, it is legitimate to ask the following question: why not use reinforcement learning to train user simulators? The answer is that in the case of dialogue management, it is easier to come up with a reasonable reward function: task completion, dialogue duration, subjective evaluation...etc... When it comes to user simulation, the objective function is how well a real user is imitated which is impossible to handcraft. Fortunately, there exists a framework where the reward function is automatically inferred from data which is particularly useful here: inverse reinforcement learning \cite{Chandramohan2011}.
        
        When it comes to incremental dialogue systems, the only existing user simulator in our knowledge is the one described in \cite{Selfridge2012b}. Its state is update every 10 ms. However, the \textit{ASR instability} phenomenon is not replicated, that is to say that the ASR hypothesis construction is monotonous whereas in reality, it is not the case. When a new audio signal increment is heard by the ASR, the output can be partially or totally modified. In this simulator, only the simple case where a new increment is added to the output is modeled.

\section{Issues and motivation}
    
    A study led by the Market Intelligence and Consulting Institute shows that the market of artificial intelligence should grow exponentially in the next decade. Virtual assistants play the main role in this domain and they are predicted to multiply their market share by 2.5. Moreover, the market is expected to multiply its turn over by 14 (30\% growth per year on average). This shows that SDSs have reached a level of maturity that enables them to significantly interest the market. Therefore, designing robust, reliable and user-friendly spoken dialogue systems raises an important issue.
    
    Nevertheless, even though virtual assistants like Siri or other more domain specific spoken dialogue systems can offer a quite reasonable quality of service for certain tasks, the quality of the interaction is still poor, making room for improvement in different areas:
    
    % HK> Rajouter une citation pour les erreurs d'ASR et la gestion des erreurs.
    \begin{itemize}
    	\item The ASR module is not perfect and the errors that it engenders are not well handled by the DM.
        \item The vocabulary is very restrained hence users often use off-domain words, especially those who are not familiar with the system.
        \item Current Natural Language Processing (NLP) techniques still do not cover a lot of discourse formulations, therefore NLU modules are often too simple to handle all the situations encountered in real dialogue.
        \item Current SDSs do not adapt to the user's profile neither to groups of users's particular behaviours (accent, culture, specific expressions...).
        \item Turn-taking is handled in a walkie-talkie manner which is too simple compared to the reality of dialogue.
    \end{itemize}
    
    In this thesis we focus on the last point: turn-taking capabilities improvement. We identified two research streams:
    
    %HK> Revoir les papiers de références ici et bien les placer.
    \begin{enumerate}
    	\item Barge-in points identification to achieve smoother and more human like turn-taking. When a barge-in point is detected, it can be interpreted as an end-point so that the system can take the floor in a more reactive way, or as a suitable point for backchanneling. In these kind of studies, prosodic features are crucial but they can also be mixed with lexical and semantic ones.
        \item Turn-taking optimisation to improve dialogue efficiency (generally measured through dialogue duration and task completion). The main objective is to improve error handling by reporting errors quickly, and to improve reactivity in general as the system can respond as soon as it has enough information to do so. Unlike the previous stream, semantic features have more importance here.
    \end{enumerate}
    
    This thesis is a contribution to the second research stream. First we try to understand what is turn-taking in human-human interaction, then we ask ourselves what phenomena can be replicated in human-machine dialogue in order to offer a better error-handling and an increased efficiency in general.
    
    
    
    
    
    
